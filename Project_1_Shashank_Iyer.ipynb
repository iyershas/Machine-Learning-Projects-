{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08owH-AVyZCX"
   },
   "source": [
    "# APS1070\n",
    "#### Basic Principles and Models - Lab 1\n",
    "**Deadline: Jan 24, 23:59 - 10 points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qonbueFGyemb"
   },
   "source": [
    "Name: **Shashank Sree Kumar Iyer**\n",
    "\n",
    "Student ID: **1004859806**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dykrw3cyy7PF"
   },
   "source": [
    "##**Marking Scheme:**\n",
    "\n",
    "This project is worth **10 marks** of your final grade.\n",
    "\n",
    "**One (1) mark** of the lab is dedicated to **vectorized coding**. If you need to write a loop in your solution, think about how you can implement the same functionality with vectorized operations. Try to avoid loops as much as possible (in some cases loops are inevitable).\n",
    "\n",
    "This notebook is composed of two sections, a Tutorial, and an Exercise. \n",
    "\n",
    "The TAs in the lab will help you to complete your tutorial (Although no mark is assigned to the **tutorial** compeleting that section is **mandatory**). \n",
    "\n",
    "**The exercise** section is worth **9 points**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwPF0KFQyZCa"
   },
   "source": [
    "##Tutorial\n",
    "In this lab, we will be using the popular machine learning library [scikit-learn](https://scikit-learn.org/stable/) in tandem with a popular scientific computing library in Python, [NumPy](https://www.numpy.org/), to investigate basic machine learning principles and models. The topics that will be covered in this lab include:\n",
    "* Introduction to scikit-learn and NumPy\n",
    "* Data preparation and cleaning with Pandas\n",
    "* Exploratory data analysis (EDA)\n",
    "* Nearest neighbors classification algorithm\n",
    "\n",
    "*Note:* Some other useful Python libraries include [matplotlib](https://matplotlib.org/) (for plotting/graphing) and [Pandas](https://pandas.pydata.org/) (for data analysis), though we won't be going into detail on these in this bootcamp. \n",
    "\n",
    "##### Jupyter Notebooks\n",
    "This lab will be using [Jupyter Notebooks](https://jupyter.org/) as a Python development environment. Hopefully you're somewhat familiar with them. Write your code in *cells* (this is a cell!) and execute your code by pressing the *play* button (up top) or by entering *ctrl+enter*. To format a cell for text, you can select \"Markdown\" from the dropdown - the default formatting is \"Code\", which will usually be what you want.\n",
    "\n",
    "#### Getting started\n",
    "Let's get started. First, we're going to test that we're able to import the required libraries.  \n",
    "**>> Run the code in the next cell** to import scikit-learn and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImUix5ZzyZCc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siJZHN9gyZCl"
   },
   "source": [
    "### NumPy Basics\n",
    "\n",
    "Great. Let's move on to our next topic: getting a handle on NumPy basics. You can think of NumPy as sort of like a MATLAB for Python (if that helps). The main object is multidimensional arrays, and these come in particularly handy when working with data and machine learning algorithms.\n",
    "\n",
    "Let's create a 2x4 array containing the numbers 1 through 8 and conduct some basic operations on it.  \n",
    "**>> Run the code in the next cell to create and print the array.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLOnl7TLyZCm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(8).reshape(2,4)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbPOCjNjyZCp"
   },
   "source": [
    "We can access the shape, number of dimensions, data type, and number of elements in our array as follows:  \n",
    "*(Tip: use \"print()\" when you want a cell to output more than one thing, or you want to append text to your output, otherwise the cell will output the last object you call, as in the cell above)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShbOGLUHyZCq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2, 4)\n",
      "Dimensions: 2\n",
      "Data type: int32\n",
      "Number of elements: 8\n"
     ]
    }
   ],
   "source": [
    "print (\"Shape:\", array.shape)\n",
    "print (\"Dimensions:\", array.ndim)\n",
    "print (\"Data type:\" , array.dtype.name)\n",
    "print (\"Number of elements:\", array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJyUJgrEyZCu"
   },
   "source": [
    "If we have a Python list containing a set of numbers, we can use it to create an array:  \n",
    "*(Tip: if you click on a function call, such as array(), and press \"shift+tab\" the Notebook will provide you all the details of the function)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opgqbuJCyZCw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  2,  3,  5,  8, 13, 21])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [0, 1, 1, 2, 3, 5, 8, 13, 21]\n",
    "myarray = np.array(mylist)\n",
    "myarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9aAe1yXyZC1"
   },
   "source": [
    "And we can do it for nested lists as well, creating multidimensional NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kE94l8bQyZC2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my2dlist = [[1,2,3],[4,5,6]]\n",
    "my2darray = np.array(my2dlist)\n",
    "my2darray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZefTZQ67yZC8"
   },
   "source": [
    "We can also index and slice NumPy arrays like we would do with a Python list or another container object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-dkbQpCyZC9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originally:  [0 1 2 3 4 5 6 7 8 9]\n",
      "First four elements:  [0 1 2 3]\n",
      "After the first four elements:  [4 5 6 7 8 9]\n",
      "The last element:  9\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(10)\n",
    "print (\"Originally: \", array)\n",
    "print (\"First four elements: \", array[:4])\n",
    "print (\"After the first four elements: \", array[4:])\n",
    "print (\"The last element: \", array[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhN9DA7myZDA"
   },
   "source": [
    "And we can index/slice multidimensional arrays, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_BlSFH1yZDB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originally:  [[1 2 3]\n",
      " [4 5 6]]\n",
      "First row only:  [1 2 3]\n",
      "First column only:  [1 4]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1,2,3],[4,5,6]])\n",
    "print (\"Originally: \", array)\n",
    "print (\"First row only: \", array[0])\n",
    "print (\"First column only: \", array[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhClSzJpyZDE"
   },
   "source": [
    "#### Sneak preview\n",
    "\n",
    "Often, when designing a machine learning classifier, it can be useful to compare an array of predictions (0 or 1 values) to another array of true values. We can do this pretty easily in NumPy to compute the *accuracy* (e.g., the number of values that are the same), for example, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3MUxHymyZDG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  70.0 %\n"
     ]
    }
   ],
   "source": [
    "true_values = [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
    "predictions = [0, 0, 0, 1, 1, 1, 0, 1, 1, 0]\n",
    "\n",
    "true_values_array = np.array(true_values)\n",
    "predictions_array = np.array(predictions)\n",
    "\n",
    "accuracy = np.sum(true_values_array == predictions_array) / true_values_array.size\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGkUlHdSyZDJ"
   },
   "source": [
    "In the previous cell, we took two Python lists, converted them to NumPy arrays, and then used a combination of np.sum() and .size to compute the accuracy (proportion of elements that are pairwise equal). A tiny bit more advanced, but demonstrates the power of NumPy arrays.\n",
    "\n",
    "You'll notice we didn't used nested loops to conduct the comparison, but instead used the np.sum() function. This is an example of a vectorized operation within NumPy that is much more efficient when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZIUpzElyZDK"
   },
   "source": [
    "### Pandas basics\n",
    "\n",
    "Pandas is an incredibly useful library that allows us to work with large datasets in Python. It contains myriad useful tools, and is highly compatible with other libraries like Scikit-learn, so you don't have to spend any time getting the two to play nicely together.\n",
    "\n",
    "First we are going to load a dataset with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8kZVXRryZDL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\k0612446\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75PRsoy0yZDO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................] 386988 / 386988"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arabica_data (5).csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "wget.download(\n",
    "    'https://github.com/alexwolson/APS1070_data/raw/master/arabica_data.csv',\n",
    "    'arabica_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13FXW71FyZDQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('arabica_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGUy-OIqyZDT"
   },
   "source": [
    "With Pandas, the main object we work with is referred to as a _DataFrame_ (hence calling our object here df). A DataFrame stores our dataset in a way that immediately gives us a lot of power to interact with it. If you just put the DataFrame in a cell on its own, you instantly get a clear, easy to read preview of the data you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzm8z_1uyZDU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Aftertaste</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Bag Weight</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Category.One.Defects</th>\n",
       "      <th>Category.Two.Defects</th>\n",
       "      <th>Clean Cup</th>\n",
       "      <th>...</th>\n",
       "      <th>Producer</th>\n",
       "      <th>Region</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Uniformity</th>\n",
       "      <th>Variety</th>\n",
       "      <th>altitude_high_meters</th>\n",
       "      <th>altitude_low_meters</th>\n",
       "      <th>altitude_mean_meters</th>\n",
       "      <th>quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.75</td>\n",
       "      <td>8.67</td>\n",
       "      <td>8.67</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>METAD PLC</td>\n",
       "      <td>guji-hambela</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2200.00</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>2075.00</td>\n",
       "      <td>90.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.58</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.75</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>METAD PLC</td>\n",
       "      <td>guji-hambela</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Other</td>\n",
       "      <td>2200.00</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>2075.00</td>\n",
       "      <td>89.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.42</td>\n",
       "      <td>1</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>1800.00</td>\n",
       "      <td>1600.00</td>\n",
       "      <td>1700.00</td>\n",
       "      <td>89.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.17</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>8.25</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Yidnekachew Dabessa Coffee Plantation</td>\n",
       "      <td>oromia</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2200.00</td>\n",
       "      <td>1800.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>89.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.25</td>\n",
       "      <td>8.25</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>8.33</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>METAD PLC</td>\n",
       "      <td>guji-hambela</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Other</td>\n",
       "      <td>2200.00</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>2075.00</td>\n",
       "      <td>88.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1306</td>\n",
       "      <td>1306</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.17</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1 kg</td>\n",
       "      <td>6.17</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Omar Acosta</td>\n",
       "      <td>marcala</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Catuai</td>\n",
       "      <td>1450.00</td>\n",
       "      <td>1450.00</td>\n",
       "      <td>1450.00</td>\n",
       "      <td>68.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1307</td>\n",
       "      <td>1307</td>\n",
       "      <td>7.42</td>\n",
       "      <td>6.25</td>\n",
       "      <td>7.08</td>\n",
       "      <td>2 kg</td>\n",
       "      <td>6.75</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>JUAN CARLOS GARCÍA LOPEZ</td>\n",
       "      <td>juchique de ferrer</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>900.00</td>\n",
       "      <td>900.00</td>\n",
       "      <td>900.00</td>\n",
       "      <td>67.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.75</td>\n",
       "      <td>69 kg</td>\n",
       "      <td>6.67</td>\n",
       "      <td>7.08</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>COEB Koperativ Ekselsyo Basen</td>\n",
       "      <td>department d'artibonite , haiti</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>6.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>Typica</td>\n",
       "      <td>350.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>63.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>1309</td>\n",
       "      <td>6.25</td>\n",
       "      <td>6.33</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1 kg</td>\n",
       "      <td>6.08</td>\n",
       "      <td>6.42</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.33</td>\n",
       "      <td>...</td>\n",
       "      <td>Teófilo Narváez</td>\n",
       "      <td>jalapa</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>Caturra</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>59.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1310</td>\n",
       "      <td>7.67</td>\n",
       "      <td>6.67</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0 lbs</td>\n",
       "      <td>6.67</td>\n",
       "      <td>7.33</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>WILLIAM ESTUARDO MARTINEZ PACHECO</td>\n",
       "      <td>nuevo oriente</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>1.33</td>\n",
       "      <td>8.00</td>\n",
       "      <td>Catuai</td>\n",
       "      <td>1417.32</td>\n",
       "      <td>1417.32</td>\n",
       "      <td>1417.32</td>\n",
       "      <td>43.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1311 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Acidity  Aftertaste  Aroma Bag Weight  Balance  Body  \\\n",
       "0              0     8.75        8.67   8.67      60 kg     8.42  8.50   \n",
       "1              1     8.58        8.50   8.75      60 kg     8.42  8.42   \n",
       "2              2     8.42        8.42   8.42          1     8.42  8.33   \n",
       "3              3     8.42        8.42   8.17      60 kg     8.25  8.50   \n",
       "4              4     8.50        8.25   8.25      60 kg     8.33  8.42   \n",
       "...          ...      ...         ...    ...        ...      ...   ...   \n",
       "1306        1306     6.50        6.17   7.00       1 kg     6.17  6.67   \n",
       "1307        1307     7.42        6.25   7.08       2 kg     6.75  7.25   \n",
       "1308        1308     6.67        6.42   6.75      69 kg     6.67  7.08   \n",
       "1309        1309     6.25        6.33   7.25       1 kg     6.08  6.42   \n",
       "1310        1310     7.67        6.67   7.50      0 lbs     6.67  7.33   \n",
       "\n",
       "      Category.One.Defects  Category.Two.Defects  Clean Cup  ...  \\\n",
       "0                        0                     0      10.00  ...   \n",
       "1                        0                     1      10.00  ...   \n",
       "2                        0                     0      10.00  ...   \n",
       "3                        0                     2      10.00  ...   \n",
       "4                        0                     2      10.00  ...   \n",
       "...                    ...                   ...        ...  ...   \n",
       "1306                     0                     4       0.00  ...   \n",
       "1307                     0                    20       6.00  ...   \n",
       "1308                     8                    16       6.00  ...   \n",
       "1309                     1                     5       1.33  ...   \n",
       "1310                     0                     4       0.00  ...   \n",
       "\n",
       "                                   Producer                           Region  \\\n",
       "0                                 METAD PLC                     guji-hambela   \n",
       "1                                 METAD PLC                     guji-hambela   \n",
       "2                                       NaN                              NaN   \n",
       "3     Yidnekachew Dabessa Coffee Plantation                           oromia   \n",
       "4                                 METAD PLC                     guji-hambela   \n",
       "...                                     ...                              ...   \n",
       "1306                            Omar Acosta                          marcala   \n",
       "1307               JUAN CARLOS GARCÍA LOPEZ               juchique de ferrer   \n",
       "1308          COEB Koperativ Ekselsyo Basen  department d'artibonite , haiti   \n",
       "1309                        Teófilo Narváez                           jalapa   \n",
       "1310      WILLIAM ESTUARDO MARTINEZ PACHECO                    nuevo oriente   \n",
       "\n",
       "      Species  Sweetness Uniformity  Variety  altitude_high_meters  \\\n",
       "0     Arabica      10.00      10.00      NaN               2200.00   \n",
       "1     Arabica      10.00      10.00    Other               2200.00   \n",
       "2     Arabica      10.00      10.00  Bourbon               1800.00   \n",
       "3     Arabica      10.00      10.00      NaN               2200.00   \n",
       "4     Arabica      10.00      10.00    Other               2200.00   \n",
       "...       ...        ...        ...      ...                   ...   \n",
       "1306  Arabica       8.00       8.00   Catuai               1450.00   \n",
       "1307  Arabica      10.00      10.00  Bourbon                900.00   \n",
       "1308  Arabica       6.00       9.33   Typica                350.00   \n",
       "1309  Arabica       6.00       6.00  Caturra               1100.00   \n",
       "1310  Arabica       1.33       8.00   Catuai               1417.32   \n",
       "\n",
       "     altitude_low_meters altitude_mean_meters quality_score  \n",
       "0                1950.00              2075.00         90.58  \n",
       "1                1950.00              2075.00         89.92  \n",
       "2                1600.00              1700.00         89.75  \n",
       "3                1800.00              2000.00         89.00  \n",
       "4                1950.00              2075.00         88.83  \n",
       "...                  ...                  ...           ...  \n",
       "1306             1450.00              1450.00         68.33  \n",
       "1307              900.00               900.00         67.92  \n",
       "1308              350.00               350.00         63.08  \n",
       "1309             1100.00              1100.00         59.83  \n",
       "1310             1417.32              1417.32         43.13  \n",
       "\n",
       "[1311 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IM8MYtTNyZDX"
   },
   "source": [
    "But even though this is printed out well, the dataset is a bit too large for this view to be anything but overwhelming. Luckily, Pandas allows us to easily get some summary statistics about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zxPSSBIyZDY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Aftertaste</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Category.One.Defects</th>\n",
       "      <th>Category.Two.Defects</th>\n",
       "      <th>Clean Cup</th>\n",
       "      <th>Cupper Points</th>\n",
       "      <th>Flavor</th>\n",
       "      <th>Moisture</th>\n",
       "      <th>Number of Bags</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Uniformity</th>\n",
       "      <th>altitude_high_meters</th>\n",
       "      <th>altitude_low_meters</th>\n",
       "      <th>altitude_mean_meters</th>\n",
       "      <th>quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.00000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>7.538764</td>\n",
       "      <td>7.403158</td>\n",
       "      <td>7.569527</td>\n",
       "      <td>7.523288</td>\n",
       "      <td>7.523387</td>\n",
       "      <td>0.450038</td>\n",
       "      <td>3.626240</td>\n",
       "      <td>9.83312</td>\n",
       "      <td>7.502441</td>\n",
       "      <td>7.523539</td>\n",
       "      <td>0.088963</td>\n",
       "      <td>153.678108</td>\n",
       "      <td>9.910900</td>\n",
       "      <td>9.839497</td>\n",
       "      <td>1808.751552</td>\n",
       "      <td>1759.456703</td>\n",
       "      <td>1784.104128</td>\n",
       "      <td>82.148825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>378.597412</td>\n",
       "      <td>0.319773</td>\n",
       "      <td>0.349945</td>\n",
       "      <td>0.315930</td>\n",
       "      <td>0.349174</td>\n",
       "      <td>0.293089</td>\n",
       "      <td>2.017571</td>\n",
       "      <td>5.482857</td>\n",
       "      <td>0.77135</td>\n",
       "      <td>0.428989</td>\n",
       "      <td>0.341817</td>\n",
       "      <td>0.047907</td>\n",
       "      <td>129.760079</td>\n",
       "      <td>0.454824</td>\n",
       "      <td>0.491508</td>\n",
       "      <td>8767.192330</td>\n",
       "      <td>8767.851565</td>\n",
       "      <td>8767.021485</td>\n",
       "      <td>2.893505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>6.170000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>6.080000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.170000</td>\n",
       "      <td>6.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>43.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>327.500000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>7.420000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>81.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.420000</td>\n",
       "      <td>7.580000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.580000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1350.000000</td>\n",
       "      <td>1310.640000</td>\n",
       "      <td>1310.640000</td>\n",
       "      <td>82.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>982.500000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.580000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1650.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>83.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1310.000000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>8.670000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>8.580000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.830000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1062.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>190164.000000</td>\n",
       "      <td>190164.000000</td>\n",
       "      <td>190164.000000</td>\n",
       "      <td>90.580000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      Acidity   Aftertaste        Aroma      Balance  \\\n",
       "count  1311.000000  1311.000000  1311.000000  1311.000000  1311.000000   \n",
       "mean    655.000000     7.538764     7.403158     7.569527     7.523288   \n",
       "std     378.597412     0.319773     0.349945     0.315930     0.349174   \n",
       "min       0.000000     5.250000     6.170000     5.080000     6.080000   \n",
       "25%     327.500000     7.330000     7.250000     7.420000     7.330000   \n",
       "50%     655.000000     7.500000     7.420000     7.580000     7.500000   \n",
       "75%     982.500000     7.750000     7.580000     7.750000     7.750000   \n",
       "max    1310.000000     8.750000     8.670000     8.750000     8.750000   \n",
       "\n",
       "              Body  Category.One.Defects  Category.Two.Defects   Clean Cup  \\\n",
       "count  1311.000000           1311.000000           1311.000000  1311.00000   \n",
       "mean      7.523387              0.450038              3.626240     9.83312   \n",
       "std       0.293089              2.017571              5.482857     0.77135   \n",
       "min       5.250000              0.000000              0.000000     0.00000   \n",
       "25%       7.330000              0.000000              0.000000    10.00000   \n",
       "50%       7.500000              0.000000              2.000000    10.00000   \n",
       "75%       7.670000              0.000000              4.000000    10.00000   \n",
       "max       8.580000             31.000000             55.000000    10.00000   \n",
       "\n",
       "       Cupper Points       Flavor     Moisture  Number of Bags    Sweetness  \\\n",
       "count    1311.000000  1311.000000  1311.000000     1311.000000  1311.000000   \n",
       "mean        7.502441     7.523539     0.088963      153.678108     9.910900   \n",
       "std         0.428989     0.341817     0.047907      129.760079     0.454824   \n",
       "min         5.170000     6.080000     0.000000        0.000000     1.330000   \n",
       "25%         7.250000     7.330000     0.090000       14.000000    10.000000   \n",
       "50%         7.500000     7.580000     0.110000      170.000000    10.000000   \n",
       "75%         7.750000     7.750000     0.120000      275.000000    10.000000   \n",
       "max        10.000000     8.830000     0.280000     1062.000000    10.000000   \n",
       "\n",
       "        Uniformity  altitude_high_meters  altitude_low_meters  \\\n",
       "count  1311.000000           1084.000000          1084.000000   \n",
       "mean      9.839497           1808.751552          1759.456703   \n",
       "std       0.491508           8767.192330          8767.851565   \n",
       "min       6.000000              1.000000             1.000000   \n",
       "25%      10.000000           1100.000000          1100.000000   \n",
       "50%      10.000000           1350.000000          1310.640000   \n",
       "75%      10.000000           1650.000000          1600.000000   \n",
       "max      10.000000         190164.000000        190164.000000   \n",
       "\n",
       "       altitude_mean_meters  quality_score  \n",
       "count           1084.000000    1311.000000  \n",
       "mean            1784.104128      82.148825  \n",
       "std             8767.021485       2.893505  \n",
       "min                1.000000      43.130000  \n",
       "25%             1100.000000      81.170000  \n",
       "50%             1310.640000      82.500000  \n",
       "75%             1600.000000      83.670000  \n",
       "max           190164.000000      90.580000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pr5RndjlyZDb"
   },
   "source": [
    "Let's say we want to zero in on a single column. This is done the same way that you access a dictionary entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amOkMl4JyZDc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Arabica\n",
       "1       Arabica\n",
       "2       Arabica\n",
       "3       Arabica\n",
       "4       Arabica\n",
       "         ...   \n",
       "1306    Arabica\n",
       "1307    Arabica\n",
       "1308    Arabica\n",
       "1309    Arabica\n",
       "1310    Arabica\n",
       "Name: Species, Length: 1311, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6aHwWANyZDf"
   },
   "source": [
    "Using this method of column access on its own returns a `series` object - think of this as a DataFrame with only one column. If you want to get the raw values however, you can simply specify this by adding `.values` after your entry. Using this, and by putting the object in a `Set` (which does not allow duplicate entries), we can quickly see all of the possible values for any column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXgYAiucyZDf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arusha',\n",
       " 'Blue Mountain',\n",
       " 'Bourbon',\n",
       " 'Catimor',\n",
       " 'Catuai',\n",
       " 'Caturra',\n",
       " 'Ethiopian Heirlooms',\n",
       " 'Ethiopian Yirgacheffe',\n",
       " 'Gesha',\n",
       " 'Hawaiian Kona',\n",
       " 'Java',\n",
       " 'Mandheling',\n",
       " 'Marigojipe',\n",
       " 'Moka Peaberry',\n",
       " 'Mundo Novo',\n",
       " 'Other',\n",
       " 'Pacamara',\n",
       " 'Pacas',\n",
       " 'Pache Comun',\n",
       " 'Peaberry',\n",
       " 'Ruiru 11',\n",
       " 'SL14',\n",
       " 'SL28',\n",
       " 'SL34',\n",
       " 'Sulawesi',\n",
       " 'Sumatra',\n",
       " 'Sumatra Lintong',\n",
       " 'Typica',\n",
       " 'Yellow Bourbon',\n",
       " nan}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['Variety'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DceE7LJeyZDj"
   },
   "source": [
    "You may notice that the final entry in this set isn't like the others - it's `nan`, which in Pandas denotes a missing entry. When working with real world datasets it's very common for entries to be missing, and there are a variety of ways of approaching a problem like this. For now, though, we are simply going to tell Pandas to drop any row that has a missing column, using the `dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1nh1-ClyZDk"
   },
   "outputs": [],
   "source": [
    "df_clean = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmFA7BOsyZDv"
   },
   "source": [
    "**YOUR TURN** How many entries did we lose by dropping all `nan`s?\n",
    "\n",
    "* What percentage of entries are left in `df_clean`? **58.73%**\n",
    "* What column had the highest number of `nan` entries? (This can be done in one line - use Google!) **Farm Name, with a count of 356 values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftS-yaWByZDx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.73 %\n",
      "Highest number of null entries in column: Farm Name , with a count of: 356\n"
     ]
    }
   ],
   "source": [
    "### Your code here\n",
    "unclean_rows = df.shape[0]\n",
    "clean_rows = df_clean.shape[0]\n",
    "percentage_left = ((unclean_rows - clean_rows)/unclean_rows)*100\n",
    "print(\"%.2f\" % percentage_left, \"%\")\n",
    "\n",
    "\n",
    "null_columns = df.columns[df.isnull().any()]\n",
    "\n",
    "print('Highest number of null entries in column:', df[null_columns].isnull().sum().idxmax(), ', with a count of:', df[null_columns].isnull().sum().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZmx1a6kyZD0"
   },
   "source": [
    "As you perform this analysis, you will probably notice that we've lost _quite a bit_ of our original data by simply dropping the `nan` values. There is another approach that we can examine, however. Instead of dropping the missing entries entirely, we can _impute_ their value using the data we do have. For a single column we can do this like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "978WpswmyZD1"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(\n",
    "    missing_values=np.nan,\n",
    "    strategy='mean',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "imp.fit(\n",
    "    df['altitude_mean_meters'].values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
    ")\n",
    "\n",
    "df['altitude_mean_meters_imputed'] = imp.transform(df['altitude_mean_meters'].values.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmFHt0myyZD3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>altitude_mean_meters</th>\n",
       "      <th>altitude_mean_meters_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2075.0</td>\n",
       "      <td>2075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2075.0</td>\n",
       "      <td>2075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2075.0</td>\n",
       "      <td>2075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1784.104128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1784.104128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1822.5</td>\n",
       "      <td>1822.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   altitude_mean_meters  altitude_mean_meters_imputed\n",
       "0                2075.0                   2075.000000\n",
       "1                2075.0                   2075.000000\n",
       "2                1700.0                   1700.000000\n",
       "3                2000.0                   2000.000000\n",
       "4                2075.0                   2075.000000\n",
       "5                   NaN                   1784.104128\n",
       "6                   NaN                   1784.104128\n",
       "7                1635.0                   1635.000000\n",
       "8                1635.0                   1635.000000\n",
       "9                1822.5                   1822.500000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['altitude_mean_meters','altitude_mean_meters_imputed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvtZNbdoyZD5"
   },
   "source": [
    "OK, great! Now we have replaced the useless NaN values with the average height. While this obviously isn't as good as original data, in a lot of situations this can be a step up from losing rows entirely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wZRacbpyZD7"
   },
   "source": [
    "Sophisticated analysis can be done in only a few lines using Pandas. Let's say that we want to get the average coffee rating by country. First, we can use the `groupby` method to automatically collect the results by country. Then, we can select the column we want - `quality_score` - and calculate its mean the same way we would using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DQSepevyZD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country of Origin\n",
       "Brazil                          82.330725\n",
       "China                           80.868000\n",
       "Colombia                        82.932000\n",
       "Costa Rica                      83.090000\n",
       "El Salvador                     82.804545\n",
       "Ethiopia                        87.792500\n",
       "Guatemala                       81.957832\n",
       "Haiti                           80.750000\n",
       "Honduras                        81.010476\n",
       "Indonesia                       81.524286\n",
       "Kenya                           85.415000\n",
       "Laos                            82.000000\n",
       "Malawi                          81.711818\n",
       "Mexico                          80.246087\n",
       "Myanmar                         80.666667\n",
       "Nicaragua                       79.333000\n",
       "Panama                          81.750000\n",
       "Peru                            77.000000\n",
       "Philippines                     80.312500\n",
       "Taiwan                          82.462895\n",
       "Tanzania, United Republic Of    82.411724\n",
       "Uganda                          83.778333\n",
       "Name: quality_score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.groupby('Country of Origin')['quality_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bj9yPF4MyZD_"
   },
   "source": [
    "This is certainly interesting, but it could be presented better. First, all of the ratings are pretty high (what's the highest and lowest rating?). Let's standardize to unit mean and variance so that we can tell the difference more easily. We'll just do that on our subset here for now, but you can apply it to the entire dataset too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GerXlhyhyZEA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country of Origin\n",
       "Brazil                          0.194625\n",
       "China                          -0.491541\n",
       "Colombia                        0.476684\n",
       "Costa Rica                      0.550802\n",
       "El Salvador                     0.416895\n",
       "Ethiopia                        2.756749\n",
       "Guatemala                       0.019701\n",
       "Haiti                          -0.546895\n",
       "Honduras                       -0.424705\n",
       "Indonesia                      -0.183677\n",
       "Kenya                           1.641462\n",
       "Laos                            0.039482\n",
       "Malawi                         -0.095705\n",
       "Mexico                         -0.783281\n",
       "Myanmar                        -0.585987\n",
       "Nicaragua                      -1.211611\n",
       "Panama                         -0.077794\n",
       "Peru                           -2.306024\n",
       "Philippines                    -0.752127\n",
       "Taiwan                          0.256626\n",
       "Tanzania, United Republic Of    0.232622\n",
       "Uganda                          0.873700\n",
       "Name: quality_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_means = df_clean.groupby('Country of Origin')['quality_score'].mean()\n",
    "mu,si = country_means.mean(), country_means.std() #Calculate the overall mean and standard deviation of the quality scores\n",
    "country_means -= mu #Subtract the mean from every entry\n",
    "country_means /= si #Divide every entry by the standard deviation\n",
    "country_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dH5x0jMtyZEC"
   },
   "source": [
    "This is a lot clearer! Finally, let's sort this list so that it's easier to compare entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lo_Kuq-MyZED"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country of Origin\n",
       "Peru                           -2.306024\n",
       "Nicaragua                      -1.211611\n",
       "Mexico                         -0.783281\n",
       "Philippines                    -0.752127\n",
       "Myanmar                        -0.585987\n",
       "Haiti                          -0.546895\n",
       "China                          -0.491541\n",
       "Honduras                       -0.424705\n",
       "Indonesia                      -0.183677\n",
       "Malawi                         -0.095705\n",
       "Panama                         -0.077794\n",
       "Guatemala                       0.019701\n",
       "Laos                            0.039482\n",
       "Brazil                          0.194625\n",
       "Tanzania, United Republic Of    0.232622\n",
       "Taiwan                          0.256626\n",
       "El Salvador                     0.416895\n",
       "Colombia                        0.476684\n",
       "Costa Rica                      0.550802\n",
       "Uganda                          0.873700\n",
       "Kenya                           1.641462\n",
       "Ethiopia                        2.756749\n",
       "Name: quality_score, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_means.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPPkd2dUyZEF"
   },
   "source": [
    "Finally, we'll look at indexing using Pandas. Let's say that we want to only look at the coffee entries from Taiwan. We can use the following syntax to identify those rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaiB3FWiyZEF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Aftertaste</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Bag Weight</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Category.One.Defects</th>\n",
       "      <th>Category.Two.Defects</th>\n",
       "      <th>Clean Cup</th>\n",
       "      <th>...</th>\n",
       "      <th>Producer</th>\n",
       "      <th>Region</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Uniformity</th>\n",
       "      <th>Variety</th>\n",
       "      <th>altitude_high_meters</th>\n",
       "      <th>altitude_low_meters</th>\n",
       "      <th>altitude_mean_meters</th>\n",
       "      <th>quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>8.25</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>50 kg</td>\n",
       "      <td>8.17</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>FANG,ZHENG-LUN 方政倫</td>\n",
       "      <td>leye, alishan township, chiayi county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Sumatra</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>86.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>8.08</td>\n",
       "      <td>7.75</td>\n",
       "      <td>8.08</td>\n",
       "      <td>15 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>LIN YEN CHIEN 林言謙</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>86.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>7.58</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.67</td>\n",
       "      <td>40 kg</td>\n",
       "      <td>7.92</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>SU CHUEN SHIAN 蘇春賢</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>84.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.92</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Wang Chao Yung 王超永</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>350.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>84.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.83</td>\n",
       "      <td>2 kg</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Kao Ming Lee</td>\n",
       "      <td>mountain ali, taiwan</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>84.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.92</td>\n",
       "      <td>5 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>黃美桃 Huang Mei Tao</td>\n",
       "      <td>國姓鄉 guoshing township</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>84.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>233</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.58</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>CHANG YU LIANG 張玉良</td>\n",
       "      <td>nanxi dist., tainan city 臺南市楠西區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>968.00</td>\n",
       "      <td>968.00</td>\n",
       "      <td>968.00</td>\n",
       "      <td>84.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>262</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.83</td>\n",
       "      <td>50 kg</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>FANG,ZHENG-LUN 方政倫</td>\n",
       "      <td>leye, alishan township, chiayi county 嘉義阿里山樂野村</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>Typica</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>83.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>269</td>\n",
       "      <td>7.92</td>\n",
       "      <td>7.92</td>\n",
       "      <td>8.00</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>張瑞宏</td>\n",
       "      <td>台中和平區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>950.00</td>\n",
       "      <td>950.00</td>\n",
       "      <td>950.00</td>\n",
       "      <td>83.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.67</td>\n",
       "      <td>18 kg</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>ZENG JIAN NAN 曾建男</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>570.00</td>\n",
       "      <td>480.00</td>\n",
       "      <td>525.00</td>\n",
       "      <td>83.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "      <td>7.58</td>\n",
       "      <td>7.58</td>\n",
       "      <td>7.67</td>\n",
       "      <td>10 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>曾如楓 &amp; 郭俊宏 (Tseng Ju Feng &amp; Kuo Jun Hong)</td>\n",
       "      <td>台南市東山區 (dongshan dist., tainan city)</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>750.00</td>\n",
       "      <td>750.00</td>\n",
       "      <td>750.00</td>\n",
       "      <td>83.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>7.92</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.50</td>\n",
       "      <td>50 kg</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>FANG,ZHENG-LUN 方政倫</td>\n",
       "      <td>leye, alishan township, chiayi county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Caturra</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>83.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.67</td>\n",
       "      <td>15 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>LIN YEN CHIEN 林言謙</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>83.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>7.58</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.92</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>蘇詠晴</td>\n",
       "      <td>南投國姓</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>83.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>483</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.83</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Tseng ju feng / Kuo jun hong 曾如楓 / 郭俊宏</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>Typica</td>\n",
       "      <td>800.00</td>\n",
       "      <td>700.00</td>\n",
       "      <td>750.00</td>\n",
       "      <td>83.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>7.58</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.92</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>雅慕伊</td>\n",
       "      <td>嘉義阿里山</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>83.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>536</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.17</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>7.58</td>\n",
       "      <td>7.67</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Wang Chao Yung 王超永</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Caturra</td>\n",
       "      <td>350.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>82.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2 kg</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>許文郎</td>\n",
       "      <td>台東太麻里</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>775.00</td>\n",
       "      <td>775.00</td>\n",
       "      <td>775.00</td>\n",
       "      <td>82.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>585</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.83</td>\n",
       "      <td>10 kg</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>林俊吉( Lin, Chun-Chi)</td>\n",
       "      <td>台南市東山區( dongshan dist., tainan city)</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>Yellow Bourbon</td>\n",
       "      <td>650.00</td>\n",
       "      <td>650.00</td>\n",
       "      <td>650.00</td>\n",
       "      <td>82.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>617</td>\n",
       "      <td>7.42</td>\n",
       "      <td>7.83</td>\n",
       "      <td>8.00</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>林道明</td>\n",
       "      <td>嘉義阿里山</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>82.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>634</td>\n",
       "      <td>7.42</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.67</td>\n",
       "      <td>5 kg</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>林文弘 Lin Wen Hong</td>\n",
       "      <td>國姓鄉 guoshing township</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>1050.00</td>\n",
       "      <td>1050.00</td>\n",
       "      <td>1050.00</td>\n",
       "      <td>82.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>770</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.67</td>\n",
       "      <td>10 kg</td>\n",
       "      <td>7.58</td>\n",
       "      <td>7.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Jufeng-Tseng 曾如楓</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>82.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>801</td>\n",
       "      <td>7.42</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.33</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Chen Jin Lin 陳金璘</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>600.00</td>\n",
       "      <td>600.00</td>\n",
       "      <td>600.00</td>\n",
       "      <td>82.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>806</td>\n",
       "      <td>7.42</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.42</td>\n",
       "      <td>10 kg</td>\n",
       "      <td>7.50</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Lin Huang, A-Mien 黃阿綿</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>Typica</td>\n",
       "      <td>700.00</td>\n",
       "      <td>600.00</td>\n",
       "      <td>650.00</td>\n",
       "      <td>82.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>844</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.67</td>\n",
       "      <td>5 kg</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>LIN SIN JI 林信吉</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>700.00</td>\n",
       "      <td>700.00</td>\n",
       "      <td>700.00</td>\n",
       "      <td>81.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>897</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.42</td>\n",
       "      <td>50 kg</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>蘇晉寬 Su Jin Kuan</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>81.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>914</td>\n",
       "      <td>7.08</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2 kg</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>劉易騰</td>\n",
       "      <td>古坑鄉荷包村尖山坑60號</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>200.00</td>\n",
       "      <td>160.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>81.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>919</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.42</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>HUANG MEI TAO 黃美桃</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>9.33</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>900.00</td>\n",
       "      <td>900.00</td>\n",
       "      <td>900.00</td>\n",
       "      <td>81.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>938</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.42</td>\n",
       "      <td>7.83</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.08</td>\n",
       "      <td>7.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>謝文品</td>\n",
       "      <td>苗栗三灣</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>110.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>81.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>962</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>50 kg</td>\n",
       "      <td>7.75</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Hu Guei Jing 胡桂青</td>\n",
       "      <td>dongshan dist., tainan city 台南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.33</td>\n",
       "      <td>Typica</td>\n",
       "      <td>680.00</td>\n",
       "      <td>680.00</td>\n",
       "      <td>680.00</td>\n",
       "      <td>81.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>1004</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.25</td>\n",
       "      <td>5 kg</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>HU KUEI CHING 胡桂青</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>758.00</td>\n",
       "      <td>758.00</td>\n",
       "      <td>758.00</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>1021</td>\n",
       "      <td>7.08</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.92</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.08</td>\n",
       "      <td>7.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>張文進</td>\n",
       "      <td>台中新社</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>550.00</td>\n",
       "      <td>550.00</td>\n",
       "      <td>550.00</td>\n",
       "      <td>80.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1080</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.58</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>曾林春英</td>\n",
       "      <td>南投國姓</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>600.00</td>\n",
       "      <td>600.00</td>\n",
       "      <td>600.00</td>\n",
       "      <td>80.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>1115</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.42</td>\n",
       "      <td>60 kg</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>簡義榮</td>\n",
       "      <td>苗栗泰安</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>850.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>80.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>1146</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.08</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>LIN REN FU 林人富</td>\n",
       "      <td>baihe dist., tainan city 臺南市白河區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>700.00</td>\n",
       "      <td>500.00</td>\n",
       "      <td>600.00</td>\n",
       "      <td>79.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1182</td>\n",
       "      <td>1182</td>\n",
       "      <td>7.25</td>\n",
       "      <td>6.83</td>\n",
       "      <td>7.08</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>7.08</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>WU SHU YI 巫叔憶</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>850.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>79.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1217</td>\n",
       "      <td>1217</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.83</td>\n",
       "      <td>20 kg</td>\n",
       "      <td>6.92</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>GUO JIUN HUNG 郭俊宏 &amp; TSENG RU FENG 曾如楓</td>\n",
       "      <td>dongshan dist., tainan city 臺南市東山區</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Typica</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>78.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1249</td>\n",
       "      <td>1249</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.67</td>\n",
       "      <td>7.00</td>\n",
       "      <td>10 kg</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9.33</td>\n",
       "      <td>...</td>\n",
       "      <td>LUIS RODRIGUEZ</td>\n",
       "      <td>oriente</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>1310.64</td>\n",
       "      <td>1310.64</td>\n",
       "      <td>1310.64</td>\n",
       "      <td>77.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Acidity  Aftertaste  Aroma Bag Weight  Balance  Body  \\\n",
       "29            29     8.25        8.00   8.00      50 kg     8.17  8.00   \n",
       "43            43     8.08        7.75   8.08      15 kg     7.83  7.75   \n",
       "108          108     7.58        8.00   7.67      40 kg     7.92  8.00   \n",
       "115          115     7.83        7.83   7.92      20 kg     7.83  7.83   \n",
       "175          175     8.00        7.75   7.83       2 kg     7.75  7.50   \n",
       "191          191     7.67        7.75   7.92       5 kg     7.83  7.67   \n",
       "233          233     7.67        7.67   7.58      20 kg     7.83  7.75   \n",
       "262          262     7.75        7.83   7.83      50 kg     7.75  7.83   \n",
       "269          269     7.92        7.92   8.00      60 kg     7.33  7.67   \n",
       "280          280     7.75        7.67   7.67      18 kg     7.75  7.42   \n",
       "298          298     7.58        7.58   7.67      10 kg     7.83  7.67   \n",
       "319          319     7.92        7.33   7.50      50 kg     7.67  7.83   \n",
       "425          425     7.50        7.50   7.67      15 kg     7.83  7.67   \n",
       "426          426     7.58        7.75   7.92      60 kg     7.33  7.50   \n",
       "483          483     7.83        7.50   7.83      60 kg     7.25  7.67   \n",
       "508          508     7.58        7.83   7.92      60 kg     7.25  7.33   \n",
       "536          536     7.67        7.25   7.17      20 kg     7.58  7.67   \n",
       "538          538     7.33        7.67   7.50       2 kg     7.75  7.50   \n",
       "585          585     7.67        7.33   7.83      10 kg     7.67  7.58   \n",
       "617          617     7.42        7.83   8.00      60 kg     7.25  7.25   \n",
       "634          634     7.42        7.50   7.67       5 kg     7.50  7.58   \n",
       "770          770     7.17        7.50   7.67      10 kg     7.58  7.17   \n",
       "801          801     7.42        7.17   7.33      20 kg     7.33  7.50   \n",
       "806          806     7.42        7.33   7.42      10 kg     7.50  8.00   \n",
       "844          844     7.25        7.17   7.67       5 kg     7.33  7.75   \n",
       "897          897     7.67        7.25   7.42      50 kg     7.25  7.33   \n",
       "914          914     7.08        7.33   7.50       2 kg     7.25  7.58   \n",
       "919          919     7.33        7.67   7.42      20 kg     7.50  7.42   \n",
       "938          938     7.17        7.42   7.83      60 kg     7.08  7.08   \n",
       "962          962     7.83        7.67   7.83      50 kg     7.75  7.83   \n",
       "1004        1004     7.25        7.17   7.25       5 kg     7.33  7.50   \n",
       "1021        1021     7.08        7.33   7.92      60 kg     7.08  7.08   \n",
       "1080        1080     7.17        7.17   7.58      60 kg     7.17  7.17   \n",
       "1115        1115     7.17        7.25   7.42      60 kg     7.00  7.00   \n",
       "1146        1146     7.00        7.25   7.08      20 kg     7.17  7.17   \n",
       "1182        1182     7.25        6.83   7.08      20 kg     7.08  7.42   \n",
       "1217        1217     7.25        7.17   6.83      20 kg     6.92  7.00   \n",
       "1249        1249     7.17        6.67   7.00      10 kg     6.83  6.83   \n",
       "\n",
       "      Category.One.Defects  Category.Two.Defects  Clean Cup  ...  \\\n",
       "29                       0                     0      10.00  ...   \n",
       "43                       0                     0      10.00  ...   \n",
       "108                      0                     0      10.00  ...   \n",
       "115                      0                     0      10.00  ...   \n",
       "175                      0                     0      10.00  ...   \n",
       "191                      0                     0      10.00  ...   \n",
       "233                      0                     4      10.00  ...   \n",
       "262                      0                     3      10.00  ...   \n",
       "269                      0                     0      10.00  ...   \n",
       "280                      0                     1      10.00  ...   \n",
       "298                      0                     0      10.00  ...   \n",
       "319                      0                     0      10.00  ...   \n",
       "425                      0                     0      10.00  ...   \n",
       "426                      0                     0      10.00  ...   \n",
       "483                      0                     0      10.00  ...   \n",
       "508                      0                     0      10.00  ...   \n",
       "536                     31                     0      10.00  ...   \n",
       "538                      0                     9      10.00  ...   \n",
       "585                      0                     0      10.00  ...   \n",
       "617                      0                     0      10.00  ...   \n",
       "634                      0                     0      10.00  ...   \n",
       "770                      0                     0      10.00  ...   \n",
       "801                      0                     0      10.00  ...   \n",
       "806                      0                     0      10.00  ...   \n",
       "844                      0                     0      10.00  ...   \n",
       "897                      0                     0      10.00  ...   \n",
       "914                      0                     2      10.00  ...   \n",
       "919                      0                     0      10.00  ...   \n",
       "938                      0                     0      10.00  ...   \n",
       "962                      0                     0      10.00  ...   \n",
       "1004                     0                     0      10.00  ...   \n",
       "1021                     0                     0      10.00  ...   \n",
       "1080                     0                     0      10.00  ...   \n",
       "1115                     0                     0      10.00  ...   \n",
       "1146                     0                     0      10.00  ...   \n",
       "1182                     0                     0      10.00  ...   \n",
       "1217                     0                     0      10.00  ...   \n",
       "1249                     0                     2       9.33  ...   \n",
       "\n",
       "                                      Producer  \\\n",
       "29                          FANG,ZHENG-LUN 方政倫   \n",
       "43                           LIN YEN CHIEN 林言謙   \n",
       "108                         SU CHUEN SHIAN 蘇春賢   \n",
       "115                         Wang Chao Yung 王超永   \n",
       "175                               Kao Ming Lee   \n",
       "191                          黃美桃 Huang Mei Tao   \n",
       "233                         CHANG YU LIANG 張玉良   \n",
       "262                         FANG,ZHENG-LUN 方政倫   \n",
       "269                                        張瑞宏   \n",
       "280                          ZENG JIAN NAN 曾建男   \n",
       "298   曾如楓 & 郭俊宏 (Tseng Ju Feng & Kuo Jun Hong)   \n",
       "319                         FANG,ZHENG-LUN 方政倫   \n",
       "425                          LIN YEN CHIEN 林言謙   \n",
       "426                                        蘇詠晴   \n",
       "483     Tseng ju feng / Kuo jun hong 曾如楓 / 郭俊宏   \n",
       "508                                        雅慕伊   \n",
       "536                         Wang Chao Yung 王超永   \n",
       "538                                        許文郎   \n",
       "585                        林俊吉( Lin, Chun-Chi)   \n",
       "617                                        林道明   \n",
       "634                           林文弘 Lin Wen Hong   \n",
       "770                           Jufeng-Tseng 曾如楓   \n",
       "801                           Chen Jin Lin 陳金璘   \n",
       "806                      Lin Huang, A-Mien 黃阿綿   \n",
       "844                             LIN SIN JI 林信吉   \n",
       "897                            蘇晉寬 Su Jin Kuan   \n",
       "914                                        劉易騰   \n",
       "919                  HUANG MEI TAO 黃美桃           \n",
       "938                                        謝文品   \n",
       "962                           Hu Guei Jing 胡桂青   \n",
       "1004                         HU KUEI CHING 胡桂青   \n",
       "1021                                       張文進   \n",
       "1080                                      曾林春英   \n",
       "1115                                       簡義榮   \n",
       "1146                            LIN REN FU 林人富   \n",
       "1182                             WU SHU YI 巫叔憶   \n",
       "1217     GUO JIUN HUNG 郭俊宏 & TSENG RU FENG 曾如楓   \n",
       "1249                            LUIS RODRIGUEZ   \n",
       "\n",
       "                                              Region  Species  Sweetness  \\\n",
       "29             leye, alishan township, chiayi county  Arabica      10.00   \n",
       "43                                      natou county  Arabica      10.00   \n",
       "108                                     natou county  Arabica      10.00   \n",
       "115               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "175                             mountain ali, taiwan  Arabica      10.00   \n",
       "191                            國姓鄉 guoshing township  Arabica      10.00   \n",
       "233                  nanxi dist., tainan city 臺南市楠西區  Arabica      10.00   \n",
       "262   leye, alishan township, chiayi county 嘉義阿里山樂野村  Arabica      10.00   \n",
       "269                                            台中和平區  Arabica      10.00   \n",
       "280               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "298             台南市東山區 (dongshan dist., tainan city)  Arabica      10.00   \n",
       "319            leye, alishan township, chiayi county  Arabica      10.00   \n",
       "425                                     natou county  Arabica      10.00   \n",
       "426                                             南投國姓  Arabica      10.00   \n",
       "483               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "508                                            嘉義阿里山  Arabica      10.00   \n",
       "536               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "538                                            台東太麻里  Arabica      10.00   \n",
       "585             台南市東山區( dongshan dist., tainan city)  Arabica      10.00   \n",
       "617                                            嘉義阿里山  Arabica      10.00   \n",
       "634                            國姓鄉 guoshing township  Arabica      10.00   \n",
       "770               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "801               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "806               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "844               dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "897                                     natou county  Arabica      10.00   \n",
       "914                                     古坑鄉荷包村尖山坑60號  Arabica      10.00   \n",
       "919                                     natou county  Arabica       9.33   \n",
       "938                                             苗栗三灣  Arabica      10.00   \n",
       "962               dongshan dist., tainan city 台南市東山區  Arabica      10.00   \n",
       "1004              dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "1021                                            台中新社  Arabica      10.00   \n",
       "1080                                            南投國姓  Arabica      10.00   \n",
       "1115                                            苗栗泰安  Arabica      10.00   \n",
       "1146                 baihe dist., tainan city 臺南市白河區  Arabica      10.00   \n",
       "1182                                    natou county  Arabica      10.00   \n",
       "1217              dongshan dist., tainan city 臺南市東山區  Arabica      10.00   \n",
       "1249                                         oriente  Arabica      10.00   \n",
       "\n",
       "     Uniformity         Variety  altitude_high_meters altitude_low_meters  \\\n",
       "29        10.00         Sumatra               1200.00             1200.00   \n",
       "43        10.00         Bourbon               1000.00             1000.00   \n",
       "108       10.00          Typica                800.00              800.00   \n",
       "115       10.00          Typica                350.00              350.00   \n",
       "175       10.00          Typica               1200.00             1200.00   \n",
       "191       10.00          Typica               1000.00             1000.00   \n",
       "233       10.00          Typica                968.00              968.00   \n",
       "262        9.33          Typica               1200.00             1200.00   \n",
       "269       10.00          Typica                950.00              950.00   \n",
       "280       10.00          Typica                570.00              480.00   \n",
       "298       10.00          Typica                750.00              750.00   \n",
       "319       10.00         Caturra               1200.00             1200.00   \n",
       "425       10.00         Bourbon               1000.00             1000.00   \n",
       "426       10.00          Typica                800.00              800.00   \n",
       "483        9.33          Typica                800.00              700.00   \n",
       "508       10.00          Typica               1200.00             1200.00   \n",
       "536       10.00         Caturra                350.00              350.00   \n",
       "538       10.00          Typica                775.00              775.00   \n",
       "585        9.33  Yellow Bourbon                650.00              650.00   \n",
       "617       10.00          Typica               1200.00             1200.00   \n",
       "634       10.00          Typica               1050.00             1050.00   \n",
       "770       10.00          Typica                800.00              800.00   \n",
       "801       10.00          Typica                600.00              600.00   \n",
       "806        9.33          Typica                700.00              600.00   \n",
       "844       10.00          Typica                700.00              700.00   \n",
       "897       10.00          Typica                800.00              800.00   \n",
       "914       10.00          Typica                200.00              160.00   \n",
       "919       10.00          Typica                900.00              900.00   \n",
       "938       10.00          Typica                110.00              110.00   \n",
       "962        9.33          Typica                680.00              680.00   \n",
       "1004      10.00          Typica                758.00              758.00   \n",
       "1021      10.00          Typica                550.00              550.00   \n",
       "1080      10.00          Typica                600.00              600.00   \n",
       "1115      10.00          Typica                850.00              850.00   \n",
       "1146      10.00          Typica                700.00              500.00   \n",
       "1182      10.00          Typica                850.00              850.00   \n",
       "1217      10.00          Typica                800.00              800.00   \n",
       "1249      10.00         Bourbon               1310.64             1310.64   \n",
       "\n",
       "     altitude_mean_meters quality_score  \n",
       "29                1200.00         86.58  \n",
       "43                1000.00         86.08  \n",
       "108                800.00         84.92  \n",
       "115                350.00         84.83  \n",
       "175               1200.00         84.42  \n",
       "191               1000.00         84.25  \n",
       "233                968.00         84.08  \n",
       "262               1200.00         83.92  \n",
       "269                950.00         83.92  \n",
       "280                525.00         83.83  \n",
       "298                750.00         83.75  \n",
       "319               1200.00         83.67  \n",
       "425               1000.00         83.25  \n",
       "426                800.00         83.25  \n",
       "483                750.00         83.08  \n",
       "508               1200.00         83.00  \n",
       "536                350.00         82.92  \n",
       "538                775.00         82.92  \n",
       "585                650.00         82.75  \n",
       "617               1200.00         82.67  \n",
       "634               1050.00         82.58  \n",
       "770                800.00         82.17  \n",
       "801                600.00         82.00  \n",
       "806                650.00         82.00  \n",
       "844                700.00         81.83  \n",
       "897                800.00         81.58  \n",
       "914                180.00         81.50  \n",
       "919                900.00         81.50  \n",
       "938                110.00         81.42  \n",
       "962                680.00         81.25  \n",
       "1004               758.00         81.00  \n",
       "1021               550.00         80.92  \n",
       "1080               600.00         80.42  \n",
       "1115               850.00         80.08  \n",
       "1146               600.00         79.75  \n",
       "1182               850.00         79.25  \n",
       "1217               800.00         78.58  \n",
       "1249              1310.64         77.67  \n",
       "\n",
       "[38 rows x 35 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[df_clean['Country of Origin'] == 'Taiwan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RD6HWUV8yZEI"
   },
   "source": [
    "Say that out of the Taiwanese coffees, we only want to look at those which are the Bourbon variety. We can also chain those indexing operations like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8P1V8rIyZEJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k0612446\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Aftertaste</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Bag Weight</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Category.One.Defects</th>\n",
       "      <th>Category.Two.Defects</th>\n",
       "      <th>Clean Cup</th>\n",
       "      <th>...</th>\n",
       "      <th>Producer</th>\n",
       "      <th>Region</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Uniformity</th>\n",
       "      <th>Variety</th>\n",
       "      <th>altitude_high_meters</th>\n",
       "      <th>altitude_low_meters</th>\n",
       "      <th>altitude_mean_meters</th>\n",
       "      <th>quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>8.08</td>\n",
       "      <td>7.75</td>\n",
       "      <td>8.08</td>\n",
       "      <td>15 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>LIN YEN CHIEN 林言謙</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>86.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.50</td>\n",
       "      <td>7.67</td>\n",
       "      <td>15 kg</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>LIN YEN CHIEN 林言謙</td>\n",
       "      <td>natou county</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>83.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1249</td>\n",
       "      <td>1249</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.67</td>\n",
       "      <td>7.00</td>\n",
       "      <td>10 kg</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9.33</td>\n",
       "      <td>...</td>\n",
       "      <td>LUIS RODRIGUEZ</td>\n",
       "      <td>oriente</td>\n",
       "      <td>Arabica</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Bourbon</td>\n",
       "      <td>1310.64</td>\n",
       "      <td>1310.64</td>\n",
       "      <td>1310.64</td>\n",
       "      <td>77.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Acidity  Aftertaste  Aroma Bag Weight  Balance  Body  \\\n",
       "43            43     8.08        7.75   8.08      15 kg     7.83  7.75   \n",
       "425          425     7.50        7.50   7.67      15 kg     7.83  7.67   \n",
       "1249        1249     7.17        6.67   7.00      10 kg     6.83  6.83   \n",
       "\n",
       "      Category.One.Defects  Category.Two.Defects  Clean Cup  ...  \\\n",
       "43                       0                     0      10.00  ...   \n",
       "425                      0                     0      10.00  ...   \n",
       "1249                     0                     2       9.33  ...   \n",
       "\n",
       "               Producer        Region  Species  Sweetness Uniformity  Variety  \\\n",
       "43    LIN YEN CHIEN 林言謙  natou county  Arabica       10.0       10.0  Bourbon   \n",
       "425   LIN YEN CHIEN 林言謙  natou county  Arabica       10.0       10.0  Bourbon   \n",
       "1249     LUIS RODRIGUEZ       oriente  Arabica       10.0       10.0  Bourbon   \n",
       "\n",
       "      altitude_high_meters altitude_low_meters altitude_mean_meters  \\\n",
       "43                 1000.00             1000.00              1000.00   \n",
       "425                1000.00             1000.00              1000.00   \n",
       "1249               1310.64             1310.64              1310.64   \n",
       "\n",
       "     quality_score  \n",
       "43           86.08  \n",
       "425          83.25  \n",
       "1249         77.67  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[df_clean['Country of Origin'] == 'Taiwan'][df_clean['Variety'] == 'Bourbon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNP5QFN5yZEO"
   },
   "source": [
    "### Scikit-learn Basics\n",
    "\n",
    "Scikit-learn is a great library to use for doing machine learning in Python. Data preparation, exploratory data analysis (EDA), classification, regression, clustering; it has it all. \n",
    "\n",
    "Scikit-learn usually expects data to be in the form of a 2D matrix with dimensions *n_samples x n_features* with an additional column for the target. To get acquainted with scikit-learn, we are going to use the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris), one of the most famous datasets in pattern recognition. \n",
    "\n",
    "Each entry in the dataset represents an iris plant, and is categorized as: \n",
    "\n",
    "* Setosa (class 0)\n",
    "* Versicolor (class 1)\n",
    "* Virginica (class 2)\n",
    "\n",
    "These represent the target classes to predict. Each entry also includes a set of features, namely:\n",
    "\n",
    "* Sepal width (cm)\n",
    "* Sepal length (cm)\n",
    "* Petal length (cm)\n",
    "* Petal width (cm)\n",
    "\n",
    "In the context of machine learning classification, the remainder of the lab is going to investigate the following question:  \n",
    "*Can we design a model that, based on the iris sample features, can accurately predict the iris sample class? *\n",
    "\n",
    "Scikit-learn has a copy of the iris dataset readily importable for us. Let's grab it now and conduct some EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nskz26U2yZEO"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "feature_data = iris_data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJ2ZaTIQyZER"
   },
   "source": [
    "**YOUR TURN:** \"feature_data\" now contains the feature data for all of the iris samples. \n",
    "* What is the shape of this feature data? **(150,4)**\n",
    "* The data type? **float64**\n",
    "* How many samples are there? **600**\n",
    "* How many features are there? **4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r53HzICUyZES"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the shape of this feature data? (150, 4)\n",
      "The data type? float64\n",
      "How many samples are there? 600\n",
      "How many features are there? 4\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here\n",
    "print(\"What is the shape of this feature data?\", feature_data.shape)\n",
    "print(\"The data type?\", feature_data.dtype)\n",
    "print(\"How many samples are there?\", feature_data.size)\n",
    "num_features = len(iris_data.feature_names)\n",
    "print(\"How many features are there?\", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9H6Z8NMyZEU"
   },
   "source": [
    "Next, we will save the target classification data in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eq1bSkNWyZEW"
   },
   "outputs": [],
   "source": [
    "target_data = iris_data.target\n",
    "target_names = iris_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-UFVjmcyZEZ"
   },
   "source": [
    "**YOUR TURN:**\n",
    "* What values are in \"target_data\"? **{0,1,2}**\n",
    "* What is the data type? **int32**\n",
    "* What values are in \"target_names\"? **{'versicolor', 'setosa', 'virginica'}**\n",
    "* What is the data type? **<U10**\n",
    "* How many samples are of type \"setosa\"? **50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSot98BYyZEb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What values are in target_data {0, 1, 2}\n",
      "What is the data type? int32\n",
      "What values are in target_names? {'versicolor', 'virginica', 'setosa'}\n",
      "What is the data type? <U10\n",
      "How many samples are of type setosa? 50\n"
     ]
    }
   ],
   "source": [
    "## Enter your code here\n",
    "print(\"What values are in target_data\", set(target_data))\n",
    "print(\"What is the data type?\", target_data.dtype)\n",
    "print(\"What values are in target_names?\", set(target_names))\n",
    "print(\"What is the data type?\", target_names.dtype)\n",
    "print(\"How many samples are of type setosa?\", list(target_data).count(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyBrtJttyZEk"
   },
   "source": [
    "We can also do some more visual EDA by plotting the samples according to a subset of the features and coloring the data points to coincide with the sample classification. We will use [matplotlib](https://matplotlib.org/), a powerful plotting library within Python, to accomplish this.\n",
    "\n",
    "For example, lets plot sepal width vs. sepal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0Z3t7QoyZEk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yptoDrGTyZEp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Visual EDA')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "setosa = feature_data[target_data==0]\n",
    "versicolor = feature_data[target_data==1]\n",
    "virginica = feature_data[target_data==2]\n",
    "\n",
    "plt.scatter(setosa[:,0], setosa[:,1], label=\"setosa\")\n",
    "plt.scatter(versicolor[:,0], versicolor[:,1], label=\"versicolor\")\n",
    "plt.scatter(virginica[:,0], virginica[:,1], label=\"virginica\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"sepal length (cm)\")\n",
    "plt.ylabel(\"sepal width (cm)\")\n",
    "plt.title(\"Visual EDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSFlhgC0yZEr"
   },
   "source": [
    "In the above step, we used boolean indexing to filter the feature data based on the target data class. This allowed us to create a scatter plot for each of the iris classes and distinguish them by color.\n",
    "\n",
    "*Observations*: We can see that the \"setosa\" class typically consists of medium-to-high sepal width with low-to-medium sepal length, while the other two classes have lower width and higher length. The \"virginica\" class appears to have the largest combination of the two. \n",
    "\n",
    "**YOUR TURN:** \n",
    "* Which of the iris classes is seperable based on sepal characteristics? **SETOSA**\n",
    "* Which of the iris classes is not? **VERSICOLOR AND VIRGINICA**\n",
    "* Can we (easily) visualize each of the samples w.r.t. all features on the same plot? Why/why not? **NO WE CANNOT, SINCE THE PLOT WITLL CONTAIN WAY TOO MANY DATA POINTS W.R.T EACH CLASS, MAKING IT EXTREMELY HARD TO VISUALIZE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCvEGfrKyZEs"
   },
   "source": [
    "### Creating a Nearest Neighbors Classifier\n",
    "\n",
    "Now that we've explored the data a little bit, we're going to use scikit-learn to create a nearest neighbors classifier for the data. Effectively we'll be developing a model whose job it is to build a relationship over input feature data (sepal and petal characteristics) that predicts the iris sample class (e.g. \"setosa\"). This is an example of a *supervised learning* task; we have all the features and all the target classes.\n",
    "\n",
    "Model creation in scikit-learn follows a **data prep -> fit -> predict** process. The \"fit\" function is where the actual model is trained and parameter values are selected, while the \"predict\" function actually takes the trained model and applies it to the new samples.\n",
    "\n",
    "First, we load the nearest neighbor library from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA1cHqdpyZEs"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64U6z3mIyZEv"
   },
   "source": [
    "Now, we're going to save our feature data into an array called 'X' and our target data into an array called 'y'. We don't *need* to do this, but it is traditional to think of the problem using this notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLaSzmgOyZEv"
   },
   "outputs": [],
   "source": [
    "X = feature_data\n",
    "y = target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_1B6hKlyZEy"
   },
   "source": [
    "Next, we create our nearest neighbor classifier object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1yoeBg_yZEz"
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE3I_-ggyZE7"
   },
   "source": [
    "And then we *fit* it to the data (i.e., train the classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyI5TqiWyZE8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DagP3Wc_yZFC"
   },
   "source": [
    "Now we have a model! If you're new to this, you've officially built your first machine learning model. If you use \"knn.predict(*[[feature array here]]*)\", you can use your trained model to predict the class of a new iris sample. \n",
    "\n",
    "**YOUR TURN:**\n",
    "* What is the predicted class of a new iris sample with feature vector [3,4,5,2]? What is its name? **2 - VERSICOLOR**\n",
    "* Do you think this model is overfit or underfit to the iris dataset? Why? **Overfit, as k was chosen arbitrarily as 1 without performing a grid fold cross validation (cross validation with hyperparameter tuning)/furthermore, we did not perform a train test split, so the data that was used to train the model was also used to test the performance of the model - thereby, not reflecting the true performance**\n",
    "* How many neighbors does our model consider when classifying a new sample? **ONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4nr0LuLyZFC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict([[3,4,5,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tipfbx0fyZFF"
   },
   "source": [
    "As you may have noted in the previous cell, we've trained this classifier on our *entire dataset*. This typically isn't done in practice and results in overfitting to the data. Here's a bit of a tricky question:\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If we use our classifier to predict the classes of the iris samples that were used to train the model itself, what will our overall accuracy be? **100%**\n",
    "\n",
    "We can validate our hypothesis fairly easily using either: i) the NumPy technique for calculating accuracy we used earlier in the lab, or ii) scikit-learn's in-house \"accuracy_score()\" function.\n",
    "\n",
    "Let's use our technique first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPMfcyB4yZFG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(target_data == knn.predict(feature_data)) / target_data.size\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djYGa2juyZFK"
   },
   "source": [
    "and then using scikit-learn's customized function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OfJOT3TyZFL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(target_data, knn.predict(feature_data))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJm79bvoyZFO"
   },
   "source": [
    "We see that our classifier has achieved 100% accuracy (and both calculation methods agree)!\n",
    "\n",
    "**DISCUSSION:** \n",
    "* Why do you think the model was able to achieve such a \"great\" result? **THE MODEL ENDED UP GETTING AN ACCURACY OF 100% BECAUSE THE DATA THAT WAS USED TO TRAIN THE MODEL, WAS USED TO TEST THE PERFORMANCE OF THE MODEL AS WELL. AS THERE WAS NO TRAIN/TEST SPLIT, THIS LED TO AN ACCURACY OF 100%**\n",
    "* What does this really tell us? **THIS ELUCIDATES THE FACT THAT THE MODEL WOULD POTENTIALLY NOT BE ABLE TO PERFORM WELL ON DATA POINTS/FEATURE DATA THAT IT HAS NEVER SEEN BEFORE.**\n",
    "* Do you expect the model to perform this well on new data? **NO IT WILL NOT PERFORM THIS WELL ON UNKNOWN DATA POINTS - A CLEAR CASE OF OVERFITTING AS OUR MODEL MEMORIZED THE TRENDS/KEY PATTERNS IN THE DATA RATHER THAN LEARNING ON HOW TO GENERALIZE THOSE TRENDS/PATTERNS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYlo7mIXyZFe"
   },
   "source": [
    "## Exercise (to be completed on your own)\n",
    "\n",
    "Let's take the tools we have learned in this lab and put them into practice on a new dataset.\n",
    "\n",
    "We're going to work with a dataset focused on diabetes. It contains a variety of health metrics for a number of patients, and then in a second object it shows whether or not that patient had diabetes. Download it using the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFo8KVcryZFe"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "diabetes_data = fetch_openml(\n",
    "    name='diabetes',\n",
    "    cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOGmwyQNyZFh"
   },
   "source": [
    "First off, take a look at the `data`, `target` and `feature_names` entires in the `diabetes_data` dictionary. They contain the information we'll be working with here. Then, create a Pandas DataFrame called `diabetes_df` containing the data and the targets, with the feature names as column headings. If you need help, refer [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for more detail on how to achieve this.\n",
    "\n",
    "* What was the average age of participants? **33.24** [0.5]\n",
    "* How many participants tested positive? How many tested negative? **POSITIVE: 268 -- NEGATIVE: 500** [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYCjqSndyZFi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the average age of participants? 33.24\n",
      "tested_negative    500\n",
      "tested_positive    268\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "import pandas as pd\n",
    "diabetes_df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "diabetes_df['Target'] = diabetes_data.target\n",
    "\n",
    "\n",
    "#What was the average age of participants?\n",
    "avg_age = diabetes_df['age'].mean()\n",
    "print(\"What was the average age of participants?\", \"%.2f\" % avg_age)\n",
    "\n",
    "#How many participants tested positive? How many tested negative?\n",
    "print(diabetes_df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27UvyCPUyZFq"
   },
   "source": [
    "The targets are currently a string representing whether or not the patient has diabetes. However, it's more useful for us if this column contains a 1 or a 0 depending on whether the patient has diabetes. Use the [Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class from Scikit-Learn to convert the labels into integers. [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WNvJzdsyZFr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg   plas  pres  skin   insu  mass   pedi   age  Target\n",
       "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0       1\n",
       "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0       0\n",
       "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0       1\n",
       "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0       0\n",
       "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0       1\n",
       "..    ...    ...   ...   ...    ...   ...    ...   ...     ...\n",
       "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0       0\n",
       "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0       0\n",
       "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0       0\n",
       "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0       1\n",
       "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0       0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "### YOUR CODE HERE\n",
    "le = LabelEncoder()\n",
    "diabetes_df['Target'] = le.fit_transform(diabetes_df['Target'])\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBmo0-W1yZFs"
   },
   "source": [
    "Now we are going to create a classifier to predict whether a patient has diabetes based on their vitals. \n",
    "\n",
    " **Note**:\n",
    " \n",
    " We should have a training, validation and test set and our main aim is to get the best accuracy on test set. While perfoming the cross-validation, the validation set is selected by the validation function each time, the input to that function should be all of dataset except your test set. \n",
    "\n",
    "So, a general guideline to follow will be first split the dataset into train and test (70:30), after this keep the test set aside untouched for final evaluation.\n",
    "Further, in the training set depending upon the number of folds for cross validation a validation set can be obtained, the model is then iteratively trained and validated on these different sets. Basically you use your training set to generate multiple splits of the Train and Validation sets.\n",
    "\n",
    "* Using `cross_val_score`, report mean cross validation accuracy on a KNN classifier with K=3 and 10 folds. Remember that the `target` column holds our labels\n",
    "\n",
    "For all the cases mentioned below test accuracy should be computed and reported:\n",
    "\n",
    "* What accuracy did the model achieve on test set?  **Accuracy of the model on the test set is: 71%** [1]\n",
    "* Find a value for K that performs better than this (Note: Try for atleast first 50 values of k). \n",
    "What value for K did you use? What was the performance ? (Note: Show some Visual Depiction to compare Model performance) **k = 17, 0.76, visually depicted in the cell below** [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsGm3b00yZFt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 1  0.68 (+/- 0.11)\n",
      "\n",
      "Accuracy for K value as 2  0.71 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 3  0.70 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 4  0.72 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 5  0.72 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 6  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 7  0.74 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 8  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 9  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 10  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 11  0.74 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 12  0.75 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 13  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 14  0.75 (+/- 0.07)\n",
      "\n",
      "Accuracy for K value as 15  0.74 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 16  0.75 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 17  0.76 (+/- 0.11)\n",
      "\n",
      "Accuracy for K value as 18  0.76 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 19  0.75 (+/- 0.11)\n",
      "\n",
      "Accuracy for K value as 20  0.75 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 21  0.75 (+/- 0.12)\n",
      "\n",
      "Accuracy for K value as 22  0.75 (+/- 0.12)\n",
      "\n",
      "Accuracy for K value as 23  0.75 (+/- 0.12)\n",
      "\n",
      "Accuracy for K value as 24  0.74 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 25  0.75 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 26  0.73 (+/- 0.11)\n",
      "\n",
      "Accuracy for K value as 27  0.74 (+/- 0.12)\n",
      "\n",
      "Accuracy for K value as 28  0.73 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 29  0.73 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 30  0.73 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 31  0.73 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 32  0.73 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 33  0.74 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 34  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 35  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 36  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 37  0.74 (+/- 0.08)\n",
      "\n",
      "Accuracy for K value as 38  0.74 (+/- 0.06)\n",
      "\n",
      "Accuracy for K value as 39  0.74 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 40  0.74 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 41  0.74 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 42  0.73 (+/- 0.09)\n",
      "\n",
      "Accuracy for K value as 43  0.73 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 44  0.73 (+/- 0.11)\n",
      "\n",
      "Accuracy for K value as 45  0.73 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 46  0.72 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 47  0.73 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 48  0.73 (+/- 0.10)\n",
      "\n",
      "Accuracy for K value as 49  0.73 (+/- 0.10)\n",
      "Accuracy on the test set is:  0.70995670995671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Comparison of Model Performance')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "### SPLITTING THE DATASET INTO TRAINING AND TESTING SETS (70/30 SPLIT)\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes_df, diabetes_df['Target'], test_size=0.3)\n",
    "x_diab = diabetes_data.data\n",
    "y_diab = diabetes_data.target\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_scores = cross_val_score(knn, x_diab, y_diab, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cross_scores.mean(), cross_scores.std() * 2))\n",
    "\n",
    "#Find a value for K that performs better than this. What value for K did you use? What was the performance?\n",
    "\n",
    "k_val = 1\n",
    "x_k = []\n",
    "y_acc = []\n",
    "while k_val < 50:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = k_val)\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    cross_scores1 = cross_val_score(knn, x_diab, y_diab, cv=10)\n",
    "    print(\"\\nAccuracy for K value as\", k_val, \" %0.2f (+/- %0.2f)\" % (cross_scores1.mean(), cross_scores1.std() * 2))\n",
    "    x_k.append(k_val)\n",
    "    y_acc.append(cross_scores1.mean())\n",
    "    k_val += 1\n",
    "\n",
    "### FITTING THE KNN CLASSIFIER WITH K = 17 AND REPORTING THE ACCURACY ON THE TEST SET\n",
    "knn_final = neighbors.KNeighborsClassifier(n_neighbors = 17)\n",
    "model = knn_final.fit(X_train, y_train)\n",
    "predictions = knn_final.predict(X_test)\n",
    "print('Accuracy on the test set is: ',metrics.accuracy_score(y_test, predictions))\n",
    "old_accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "\n",
    "### VISUAL DEPICTION OF THE HYPERPARAMETER TUNING PERFORMED WITH 10 FOLD CROSS VALIDATION - TO CHOOSE THE OPTIMAL VALUE FOR K FOR THE KNN CLASSIFIER\n",
    "plt.scatter(x_k,y_acc)\n",
    "plt.xlabel(\"Values for k\")\n",
    "plt.ylabel(\"Average accuracies of the model at different k \")\n",
    "plt.title(\"Comparison of Model Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jp9EDL-FyZFv"
   },
   "source": [
    "Take a look at the `skin` feature.\n",
    "\n",
    "* According to the dataset description in `diabetes_data['DESCR']`, what does this feature represent? **Triceps skin fold thickness (mm)\\n** [0.25]\n",
    "* Are there any unusual entries in this column? If so, why? **Yes, contains a lot of missing values which have been given a value of 0.00** [0.25]\n",
    "\n",
    "Use the `SimpleImputer` class from scikit-learn to impute missing values for the `skin` and `insu` columns. Overwrite the existing `skin` and `insu` columns with these new values.[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JEhDq5lyZFv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.00000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg   plas  pres      skin        insu  mass   pedi   age  Target\n",
       "0     6.0  148.0  72.0  35.00000  155.548223  33.6  0.627  50.0       1\n",
       "1     1.0   85.0  66.0  29.00000  155.548223  26.6  0.351  31.0       0\n",
       "2     8.0  183.0  64.0  29.15342  155.548223  23.3  0.672  32.0       1\n",
       "3     1.0   89.0  66.0  23.00000   94.000000  28.1  0.167  21.0       0\n",
       "4     0.0  137.0  40.0  35.00000  168.000000  43.1  2.288  33.0       1\n",
       "..    ...    ...   ...       ...         ...   ...    ...   ...     ...\n",
       "763  10.0  101.0  76.0  48.00000  180.000000  32.9  0.171  63.0       0\n",
       "764   2.0  122.0  70.0  27.00000  155.548223  36.8  0.340  27.0       0\n",
       "765   5.0  121.0  72.0  23.00000  112.000000  26.2  0.245  30.0       0\n",
       "766   1.0  126.0  60.0  29.15342  155.548223  30.1  0.349  47.0       1\n",
       "767   1.0   93.0  70.0  31.00000  155.548223  30.4  0.315  23.0       0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "diabetes_data['DESCR']\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(\n",
    "    missing_values=0,\n",
    "    strategy='mean',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "imp.fit(\n",
    "    diabetes_df['skin'].values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
    ")\n",
    "\n",
    "diabetes_df['skin'] = imp.transform(diabetes_df['skin'].values.reshape((-1,1)))\n",
    "\n",
    "\n",
    "imp1 = SimpleImputer(\n",
    "    missing_values=0,\n",
    "    strategy='mean',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "imp1.fit(\n",
    "    diabetes_df['insu'].values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
    ")\n",
    "\n",
    "diabetes_df['insu'] = imp1.transform(diabetes_df['insu'].values.reshape((-1,1)))\n",
    "\n",
    "\n",
    "diabetes_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ji8wI48ZyZFy"
   },
   "source": [
    "Re-split the data and fit a new classifier. [2.5]\n",
    "\n",
    "\n",
    "Note: Test accuracy should be computed and reported below\n",
    "**TEST ACCURACY POST IMPUTATION IS: 76.19%**\n",
    "* Is performance better or worse with imputed values? Why might this be? (Note: Show some Visual Depiction to compare Model performance) **THE PERFORMANCE OF THE NEW CLASSIFIER WITH K=17 IMRPOVED. THE ACCURACY IS 76.19% AS COMPARED TO THE 71% WE GOT PREVIOUSLY. IMPUTING THE MISSING VALUES DEFINITELY IMPROVED THE PERFORMANCE OF THE MODEL AS THERE ARE NO MISSING VALUES/NANS ANYMORE.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgCWLMF3yZFy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is:  0.7619047619047619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Comparison of Model Performance')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5gcVZnH8e/PQAC5Cgm7kgsECCpXkSGsiooXNFEEdHEliBpFs+4a0EVUcNkY4x0v4K5hNa6oIBCRRQkaBBGiAoKZQLgkiGYDkiEoA4Q7CIF3/zhnSKXTM10zmepkUr/P8/STrlOnqt6u1PTbdarOKUUEZmZWX89b3wGYmdn65URgZlZzTgRmZjXnRGBmVnNOBGZmNedEYGZWc04E1laS3iXp8vUdRw9JW0i6RNJDkn7cxu3Ol/SBknVD0u5Vx5S39S+S/irpUUk7tGObtv45EQxRko6R1Jn/YO+RdKmkg9d3XK1ExLkR8cb1HUfBUcDfATtExDsaZ0qakb+IT2go/2gun9GmOJvKCeXJfBzcJ+kiSS8c4Lo2Bb4OvDEitoqI+wc3WttQOREMQZJOBM4AvkD6EhsLnAkcsT7jakXSJus7hiZ2Bv4YEav6qPNH4L0NZe/J5RuCaRGxFbAHsB1wen9XkP9v/g7YHFg8gOUlyd8nQ5T/44YYSdsCM4EPR8RFEfFYRDwdEZdExMdznc0knSFpRX6dIWmzPO8QSV2SPiHp3nw2caSkN0v6o6QHJH2qsL0Zki6U9CNJj0i6QdJ+hfknS/q/PG+JpLcV5k2RdI2k0yU9AMzIZVfn+crz7s1NMzdL2rvnc0o6W1K3pD9LOrXni6ZnHZK+KmmlpDskTepjn70k/3J+UNJiSYfn8s8A04F35l/Ux/WyigXA8yXtlZfbC9gilxe380FJS/M+nCtpp8K8QyX9IX/ObwJqWPb9km7Ln+cySTv39nl6ExEPAP8L9OzDzfI+uis393xL0hZ5Xs9x8ElJfwHOAW7Pq3pQ0pW53iskLchxL5D0ikLM8yV9XtI1wOPArrnsc5Kuzfv0Ekk7SDpX0sN5HbsU1vENScvzvIWSXlWYN0PSBfk4eCT/33UU5o9ROgPqlnR/3q+Dtj9rJSL8GkIvYCKwCtikjzozgeuAHYGRwLXAZ/O8Q/Ly04FNgQ8C3cB5wNbAXsCTwK65/gzgaVITyqbAScAdwKZ5/juAnUg/Kt4JPAa8MM+bkrd1PLAJ6ctzCnB1nv8mYCHpV6yAlxSWPRu4OMe0C+nX93GF9T6dYx8G/AuwAlCTfbEpsBT4FDAceB3wCPCiwuf7YR/7cgbww7z8l3PZacApuXxGLnsdcB/wMmAz4L+A3+R5I4CHC/vw3/J++UCef2SO8SV5P50KXFuIIYDde4lvfmE9I4ArgXPy9BnAXGD7vB8vAb7YcBx8Oce7Rd7PQT628nIrgXfnuCbn6R0K276LdMxskj/b/PxZdgO2BZbk/7s35DpnA98rxH8ssEOe9zHgL8DmhX3/JPDm/P/8ReC6PG8YcBPp7GdL0pnMwWX2p19NjqP1HYBf/fwPg3cBf2lR5/+ANxem3wTcmd8fAjwBDMvTW+c//oMK9RcCR+b3M3r++PL084B7gFf1su1FwBH5/RTgrob5U1idCF6XvyT+AXheoc4w4G/AnoWyfwbmF9axtDDv+fkz/H2TeF6Vv1yK6z+f1V/gMyiXCMbmL71N879jWDMRfBc4rbDcVqRktQupGam4DwV0sfoL/FJykivs48eBnfN0q0TwOPAgcDdwLin5i5SUdyvUfTlwR+E4eIr8pZvLdmHNRPBu4PcN2/sdMKWw7ZlN4vn3wvTXgEsL028FFvWxv1cC+xX2/RWFeXsCTxQ+SzdNfhC12p9+rf1y09DQcz8wQn23t+8E/Lkw/edc9tw6IuKZ/P6J/O9fC/OfIH2R9Vje8yYiniV9ie0EIOk9khblZpcHSc0SI5ot2ygirgS+CcwC/ipptqRt8vLDm3yGUYXpvxTW83h+W4y5x07A8hx3b+tqKSLuIv3K/ALwp4ho/Fxr7POIeJT0fzWqJ4bCvGDN/bIz8I3CPnyA9EVeNsYTImK7iBgVEe+KiG5SMng+sLCw3l/k8h7dEfFkH+ttPI5g7X3X7P+38Vjq9diS9LHchPNQjnFb1jx+/lJ4/ziweT72xwB/jubXdtZ1f9aOE8HQ8zvS6fKRfdRZQfpj6DE2lw3UmJ43uZ1+NLAit7t+B5hGai7YDriVNdu/+xzeNiL+MyIOIDUv7AF8nNTE8nSTz3D3AGJfAYzRmhcyB7qus0nNF2f3sp3n4pW0JanJ427SGVRxH6o4Tfoy/ef8Zd7z2iIirh1AjD3uI33p7lVY57aRLir3aDX0cONxBGvvuwEPX5yvB3wS+CfgBfn4eYiG6ye9WA6M7eUHURX7c6PmRDDERMRDpPb9WUoXeZ8vaVNJkySdlqudD5wqaaSkEbn+D9dhswdIenv+o/soqdnmOlLbbJBO0ZH0PvKFyjIkHSjpIKXbFh8jJbhn8tnKBcDnJW2dE86JA/wM1+d1fyLvp0NIzRNzBrCuHwFvzLE1Og94n6SXKl2Y/wJwfUTcCfwc2KuwD08A/r6w7LeAUwoXo7eVtNatrP2Rz4C+A5wuace83lGS3tSP1cwD9lC6VXkTSe8kNc/8bF1iK9iadJ2iG9hE0nRgm5LL/p6UYL8kaUtJm0t6ZZ436PtzY+dEMARFxNdJX4ynkv6IlpN+lf80V/kc0AncDNwC3JDLBupi0oXgnguHb490p9ISUhvw70in//sA1/RjvduQvqxWkpoc7ge+mucdT/oCXwZcTfqiPau/gUfEU8DhwCTSr+QzgfdExB8GsK4nIuKKiHiiybxfAf9BumvnHtLF0qPzvPtIF9W/RPqM4ynsp4j4Cemi7RxJD5POqnq9C6ofPklqzrour/cK4EVlF47Uj+Aw0lnQ/cAngMPy5xkMl5Ha8/9I+v9/kj6aEhtie4aU0HcnXbPpIh2jVe7PjZbyxRSzppQ6TO0eEceu71jMrBo+IzAzq7lKE4GkiZJuV+pkc3KT+WMlXSXpRqXORG+uMh4zM1tbZU1DkoaR2v4OJbXfLQAm53blnjqzgRsj4r8l7QnMi4hdKgnIzMyaqvKMYAKp08+yfMFuDmuPhROsvktgW9btFkczMxuAKgcBG8WadwB0AQc11JkBXC7peNKtiG9otiJJU4GpAFtuueUBL37xiwc9WDOzjdnChQvvi4iRzeZVmQiadQppbIeaDHw/Ir4m6eXAOZL2bugFSkTMBmYDdHR0RGdnZyUBm5ltrCQ19hJ/TpVNQ12s2XtyNGs3/RxH7pwTEb8jDRw1AjMza5sqE8ECYLykcZKGkzrXzG2ocxfwekhDBZMSQXeFMZmZWYPKEkEeDGoaqffgbcAFEbFY0kzl8eBJPRY/KOkm0rAIU8I93MzM2qrSJ0ZFxDzSeCXFsumF90uAVzYuZ2Zm7eOexWZmNedEYGZWc04EZmY150RgZlZzTgRmZjVX6V1DZtY/u5z88/Udgm3A7vzSWypZr88IzMxqzonAzKzmnAjMzGrOicDMrOacCMzMas6JwMys5pwIzMxqzonAzKzmnAjMzGrOicDMrOacCMzMas6JwMys5ipNBJImSrpd0lJJJzeZf7qkRfn1R0kPVhmPmZmtrbLRRyUNA2YBhwJdwAJJc/NzigGIiH8r1D8e2L+qeMAjO1rfqhrZ0WxDV+UZwQRgaUQsi4ingDnAEX3UnwycX2E8ZmbWRJWJYBSwvDDdlcvWImlnYBxwZYXxmJlZE1UmAjUpi17qHg1cGBHPNF2RNFVSp6TO7u7uQQvQzMyqTQRdwJjC9GhgRS91j6aPZqGImB0RHRHRMXLkyEEM0czMqkwEC4DxksZJGk76sp/bWEnSi4AXAL+rMBYzM+tFZYkgIlYB04DLgNuACyJisaSZkg4vVJ0MzImI3pqNzMysQpU+vD4i5gHzGsqmN0zPqDIGMzPrm3sWm5nVnBOBmVnNORGYmdWcE4GZWc05EZiZ1ZwTgZlZzTkRmJnVnBOBmVnNORGYmdWcE4GZWc05EZiZ1ZwTgZlZzTkRmJnVnBOBmVnNORGYmdWcE4GZWc05EZiZ1ZwTgZlZzTkRmJnVXKWJQNJESbdLWirp5F7q/JOkJZIWSzqvynjMzGxtlT28XtIwYBZwKNAFLJA0NyKWFOqMB04BXhkRKyXtWFU8ZmbWXJVnBBOApRGxLCKeAuYARzTU+SAwKyJWAkTEvRXGY2ZmTVSZCEYBywvTXbmsaA9gD0nXSLpO0sRmK5I0VVKnpM7u7u6KwjUzq6cqE4GalEXD9CbAeOAQYDLwP5K2W2uhiNkR0RERHSNHjhz0QM3M6qzKRNAFjClMjwZWNKlzcUQ8HRF3ALeTEoOZmbVJy0QgabSkkyRdLGmBpN9IOlPSWyT1tfwCYLykcZKGA0cDcxvq/BR4bd7OCFJT0bKBfRQzMxuIPhOBpO8BZwFPAV8mNd/8K3AFMBG4WtKrmy0bEauAacBlwG3ABRGxWNJMSYfnapcB90taAlwFfDwi7l/3j2VmZmW1un30axFxa5PyW4GL8i/9sb0tHBHzgHkNZdML7wM4Mb/MzGw96POMoFkSkLSbpH3y/KciYmlVwZmZWfX61aFM0qeAfYBnJT0bEe+uJiwzM2uXVtcIjs89hHvsFxGTI+JdwH7VhmZmZu3Q6q6hlcAvJL01T18u6deSfku60GtmZkNcq2sEPwTeCrxU0sVAJzAJOCwiPt6G+MzMrGJlOpTtBvwI+GfS7aBnAFtUGZSZmbVPnxeLJX0/19kC+L+I+KCk/YHvSPp9RHy2DTGamVmFWt01tH9E7Acg6UaAiLgReKukxpFEzcxsCGqVCH4h6dfAcGCNh8ZExMWVRWVmZm3TZyKIiE9K2gZ4NiIebVNMZmbWRq36ERwLPNpbEsi9jA+uJDIzM2uLVk1DOwA3SloILAS6gc2B3YHXAPcBTZ9FbGZmQ0OrpqFvSPom8DrglcC+wBOk0UTfHRF3VR+imZlVqeVYQxHxDPDL/DIzs41MlU8oMzOzIcCJwMys5pwIzMxqrlQikPQRSdso+a6kGyS9sergzMysemXPCN4fEQ8DbwRGAu8DvtRqIUkTJd0uaamktW4zlTRFUrekRfn1gX5Fb2Zm66zsE8qU/30z8L2IuEmS+lwgPdBmFnAo0AUskDQ3IpY0VP1RREzrT9BmZjZ4yp4RLJR0OSkRXCZpa+DZFstMAJZGxLKIeAqYA3igOjOzDUzZRHAcqQfxgRHxOGkQuve1WGYUsLww3ZXLGv2jpJslXShpTLMVSZoqqVNSZ3d3d8mQzcysjLKJIIA9gRPy9JakoSb60qzpKBqmLwF2iYh9gSuAHzTdeMTsiOiIiI6RI0eWDNnMzMoomwjOBF4OTM7Tj5Da//vSBRR/4Y8GVhQrRMT9EfG3PPkd4ICS8ZiZ2SApmwgOiogPA08CRMRKUvNQXxYA4yWNkzQcOBqYW6wg6YWFycNJYxiZmVkblb1r6Ol8F1AASBpJi4vFEbFK0jTgMmAYcFZELJY0E+iMiLnACZIOB1YBDwBTBvYxzMxsoMomgv8EfgLsKOnzwFHAqa0Wioh5wLyGsumF96cAp5SO1szMBl2pRBAR5+ZnEryedBH4yIhwM46Z2Uagz0QgaZuIeFjS9sC9wPmFedtHxANVB2hmZtVqdUZwHnAY6elkxVs/lad3rSguMzNrk1ZPKDss/zuuPeGYmVm7lR199G2Sti1MbyfpyOrCMjOzdinbj+DTEfFQz0REPAh8upqQzMysncomgmb1yt56amZmG7CyiaBT0tcl7SZpV0mnky4gm5nZEFc2ERwPPAX8CPgxaaiJD1cVlJmZtU/ZDmWPkYahNjOzjUypRJDHFvoEsBeF4acj4nUVxWVmZm1StmnoXOAPwDjgM8CdpNFFzcxsiCubCHaIiO8CT0fEryPi/cA/VBiXmZm1SelhqPO/90h6C+kBM6OrCcnMzNqpbCL4XO5Z/DHgv4BtgH+rLCozM2ublokgP5BmfET8DHgIeG3lUZmZWdu0vEYQEc+QHiNpZmYbobJNQ9dK+iapQ9ljPYURcUMlUZmZWduUTQSvyP/OLJQF0Gc/AkkTgW+Qnln8PxHxpV7qHUXqsXxgRHSWjMnMzAZB2Z7F/b4ukK8tzAIOBbqABZLmRsSShnpbAycA1/d3G2Zmtu7K9iye3qw8ImY2K88mAEsjYllexxzgCGBJQ73PAqcBJ5WJxczMBlfZDmWPFV7PAJOAXVosMwpYXpjuymXPkbQ/MCbfkdQrSVMldUrq7O7uLhmymZmVUbZp6GvFaUlfBea2WEzNVlVYx/OA04EpJbY/G5gN0NHRES2qm5lZP5Q9I2j0fFo/uL4LGFOYHk3qkdxja2BvYL6kO0lDVsyV1DHAmMzMbADKXiO4hdW/5ocBI1nzDqJmFgDjJY0D7gaOBo7pmZkffTmisI35wEm+a8jMrL3K3j56WOH9KuCvEbGqrwUiYpWkacBlpORxVkQsljQT6IyIVk1LZmbWBmUTwQuBxRHxCICkrSTtFRF93vIZEfOAeQ1lvd2BdEjJWMzMbBCVvUbw38CjhenHc5mZmQ1xZROBIuK5u3Ui4lnKn02YmdkGrGwiWCbpBEmb5tdHgGVVBmZmZu1RNhF8iDTe0N2k20IPAqZWFZSZmbVP2Q5l95Ju/zQzs41MqTMCST+QtF1h+gWSzqouLDMza5eyTUP7RsSDPRMRsRLYv5qQzMysncomgudJekHPhKTt8V1DZmYbhbJf5l8jPaXsQtJQE/8EfKGyqMzMrG3KXiw+W1In6YlkAt7e+IAZMzMbmko37+Qv/iWStgTeJukrEfGW6kIzM7N2KHvX0HBJR0q6ALgHeD3wrUojMzOztujzjEDSocBk4E3AVcA5wISIeF8bYjMzszZo1TR0GfBb4OCIuANA0jcqj8rMzNqmVSI4gNSj+ApJy4A5pGcLmJnZRqLPawQRcWNEfDIidgNmkDqRDZd0qSSPNWRmthEo/cziiLgmIqYBo4AzgJdXFpWZmbVNv3sH52cRXJZfZmY2xJU+IxgISRMl3S5pqaSTm8z/kKRbJC2SdLWkPauMx8zM1lZZIpA0DJgFTAL2BCY3+aI/LyL2iYiXAqcBX68qHjMza65VP4Lt+5ofEQ/0MXsCsDQiluV1zQGOAJ4bmiIiHi7U35I0jpGZmbVRq2sEC0lfzmoyL4Bd+1h2FLC8MN3zZLM1SPowcCIwnDSW0VryHUpTAcaOHdsiZDMz648+E0FEjFuHdfeWPBq3MQuYJekY4FTgvU3qzAZmA3R0dPiswcxsEJUda0iSjpX0H3l6rKQJLRbrAsYUpkcDK/qoPwc4skw8ZmY2eMpeLD6T1G/gmDz9COlCcF8WAOMljZM0nNRDeW6xgqTxhcm3AH8qGY+ZmQ2Ssv0IDoqIl0m6EdKjKvOXe68iYpWkaaT+BsOAsyJisaSZQGdEzAWmSXoD8DSwkibNQmZmVq2yieDpfDtoAEgaCTzbaqGImAfMayibXnj/kfKhmplZFco2Df0n8BNgR0mfB67Gj6o0M9solH1U5bmSFpIeSCPgyIi4rdLIzMysLfrToexe4PzivBYdyszMbAjoT4eysaQLugK2A+4C1qWfgZmZbQBaPY9gXETsSrrz560RMSIidgAOAy5qR4BmZlatsheLD8x3AAEQEZcCr6kmJDMza6eyt4/eJ+lU4IekpqJjgfsri8rMzNqm7BnBZGAk6RbSnwI75jIzMxviyt4++gDwEUnbAM9GxKPVhmVmZu1SdtC5ffLwErcAiyUtlLR3taGZmVk7lG0a+jZwYkTsHBE7Ax8jDwttZmZDW9lEsGVEXNUzERHzSU8UMzOzIa7sXUPL8rMIzsnTxwJ3VBOSmZm1U9kzgveT7hq6iHTn0EjgfVUFZWZm7VP2rqGVwAkVx2JmZutBq0Hn5vY1PyIOH9xwzMys3VqdEbwcWE4adfR6mj+Q3szMhrBWieDvgUNJvYiPAX4OnB8Ri6sOzMzM2qPV6KPPRMQvIuK9wD8AS4H5ko4vs3JJEyXdLmmppJObzD9R0hJJN0v6laSdB/QpzMxswFreNSRpM0lvJw0492HSYytbDkGdn3E8C5gE7AlMlrRnQ7UbgY6I2Be4EDitf+Gbmdm6anWx+AfA3sClwGci4tZ+rHsCsDQiluV1zQGOAJb0VCh2UgOuI/VPMDOzNmp1jeDdwGPAHsAJ0nPXigVERGzTx7KjSBeae3QBB/VR/zhSwlmLpKnAVICxY8e2CNnMzPqjz0QQEWU7nDXT7A6jaFpROhbooJeH3UTEbPLYRh0dHU3XYWZmA1N2iImB6ALGFKZHAysaK0l6A/DvwGsi4m8VxmNmZk2syy/+VhYA4yWNkzQcOBpYo4OapP1JI5seHhH3VhiLmZn1orJEEBGrgGmkB9/fBlwQEYslzZTU0yP5K8BWwI8lLWrVk9nMzAZflU1D5Afez2som154/4Yqt29mZq1V2TRkZmZDgBOBmVnNORGYmdWcE4GZWc05EZiZ1ZwTgZlZzTkRmJnVnBOBmVnNORGYmdWcE4GZWc05EZiZ1ZwTgZlZzTkRmJnVnBOBmVnNORGYmdWcE4GZWc05EZiZ1ZwTgZlZzVWaCCRNlHS7pKWSTm4y/9WSbpC0StJRVcZiZmbNVZYIJA0DZgGTgD2ByZL2bKh2FzAFOK+qOMzMrG9VPrx+ArA0IpYBSJoDHAEs6akQEXfmec9WGIeZmfWhyqahUcDywnRXLjMzsw1IlYlATcpiQCuSpkrqlNTZ3d29jmGZmVlRlYmgCxhTmB4NrBjIiiJidkR0RETHyJEjByU4MzNLqkwEC4DxksZJGg4cDcytcHtmZjYAlSWCiFgFTAMuA24DLoiIxZJmSjocQNKBkrqAdwDflrS4qnjMzKy5Ku8aIiLmAfMayqYX3i8gNRmZmdl64p7FZmY150RgZlZzTgRmZjXnRGBmVnNOBGZmNedEYGZWc04EZmY150RgZlZzTgRmZjXnRGBmVnNOBGZmNedEYGZWc04EZmY150RgZlZzTgRmZjXnRGBmVnNOBGZmNedEYGZWc04EZmY1V2kikDRR0u2Slko6ucn8zST9KM+/XtIuVcZjZmZrqywRSBoGzAImAXsCkyXt2VDtOGBlROwOnA58uap4zMysuSrPCCYASyNiWUQ8BcwBjmiocwTwg/z+QuD1klRhTGZm1mCTCtc9ClhemO4CDuqtTkSskvQQsANwX7GSpKnA1Dz5qKTbK4m4fkbQsK/rTD4f3RD5GC1Yx2N0595mVJkImv2yjwHUISJmA7MHIyhbTVJnRHSs7zjMeuNjtD2qbBrqAsYUpkcDK3qrI2kTYFvggQpjMjOzBlUmggXAeEnjJA0HjgbmNtSZC7w3vz8KuDIi1jojMDOz6lTWNJTb/KcBlwHDgLMiYrGkmUBnRMwFvgucI2kp6Uzg6Krisabc3GYbOh+jbSD/ADczqzf3LDYzqzknAjOzmnMiWAeSdpF063rc/sJ8Ib5M3f9p0rN7sOOZImmn/tZrR2yWrO9jNsewk6QLK97GdpL+tb/12hHbhsiJYIjK4zLdnXtttxQRH4iIJZUGBVOAlomgsV6bYrMNRESsiIijKt7MdkDLRNBYr02xbXCcCAaJpF0l3SjpwPyL9yJJv5D0J0mnFeo9Kunzkm6SdJ2kv2uyrlvyLxVJul/Se3L5OZLekKtNAn6RyyfnZW6Vmvc9lDRfUkchhi/nM4orJE3I85dJOjzXmSLp4vwZbpf06Vy+xi9KSSdJmiHpKKADOFfSIklbSJouaUGOa3b+PM3qFWNr+lnK7Dfrn/VwzPbUfe4Yytv9qaRLJN0haZqkE3Nc10naPtebL+kMSdfmY2NCLp8h6aTCum/NP5K+BOyWj7GvSNpK0q8k3ZBj7RnuprFeMbbNJX0v179R0msLMTfdV0NWRPg1wBewC3Ar8CLgRuCluXwKsIzUQW5z4M/AmDwvgLfm96cBpzZZ77eAtwB7k/pjfCeX/wnYKr+/GNiV9Mv6LmAk6XbgK4Ejm6xzPtBRiGFSfv8T4HJgU2A/YFHhM9xDGvJji/w5O3o+c2G9JwEzGreRp7cvvD+n8Lkb683P6+71s5TZb35t2MdsYwyF7S4Fts7/7w8BH8rzTgc+WjhGetb56sLyM4CTCuu+Na+/8TjdBNgmvx+Rt6km9YqxfQz4Xn7/4nxsbt7XvhqqL58RrLuRpC/lYyNiUaH8VxHxUEQ8CSxh9TgfTwE/y+8Xkg68Rr8lHeyvBv4b2EfSKOCBiHhU6brA6IhYBhwIzI+I7ohYBZybl+vLU+SzCeAW4NcR8XR+X4znlxFxf0Q8AVwEHNxivY1eqzS8+C3A64C9WtTv67OU2W9WTtuP2RbxXBURj0RENykRXJLLG4/H8wEi4jfANpK2a/VBCwR8QdLNwBWkcc5anVUeTPoBQ0T8gfSFv0ee19u+GpKcCNbdQ6SB817ZUP63wvtnWN157+nIPzEayot+A7wqv+YD3aSe17/N818FXJ3fD2S01mIMz/bEGhHPNsTT2MkkgFWsedxs3mwDkjYHzgSOioh9gO/0Vre4WMmYe9tvVs76OGb7Utzus4XpQTsegXeREuABEfFS4K991O3R1/HY274akpwI1t1TwJHAeyQdMxgrjIjlpNPX8flX/9WkJpieP6qJwKX5/fXAaySNUHoGxGTg14MRB3CopO0lbUH6jNeQ/oB2lLSDpM2Awwr1HyGd4sPqP7L7JG1F+lJoVq+oys9iq62PY3YwvBNA0sHAQxHxEHAn8LJc/jJgXK7beIxtC9wbEU/ntv6de6lX9BtSAkHSHsBYYKMc+XhIZ7ENRUQ8Jukw4JeSHhuk1V5PGpoD0h/TF1l9FnAIMD1v+x5JpwBXkX7BzIuIiwcphqtJp8a7A+dFRCeA0jAh1wN3AH8o1P8+8C1JTwAvJ50F3EL6Y13QRz3a8FmsYD0cs4NhpaRrgW2A9+ey/yUltEWkY+yPABFxv6Rr8oXfS0kPvbpEUiewiHzcNqk3q7C9M0nH6dJkcgkAAAMHSURBVC2kM48pEfE3bYSPTPEQE0OMpNGki2aTKt7OFNIF3WlVbsesDEnzSReFO9d3LBsjnxEMMRHRRbp11MxsUPiMwMys5nyx2Mys5pwIzMxqzonAzKzmnAhsoyEpJJ1TmN5EUrekn/W1XJP13ClpxEDq5PL/LUwfJen7/dm+Wbs5EdjG5DFg79wBDuBQ4O71EEeHpFbDaZhtMJwIbGNzKWnwM0g9k8/vmZF7Sf9U0s15ZMt9c/kOki7PI0x+m8LQApKOlfT7PDrlt3OP51a+CnyqsVBplNdr83aulfSiXF52BM7d8oiXCyX9VtKLc/k78qibN0n6zcB2m9WZE4FtbOYAR+exjvYl9Xbt8RngxojYl/RFfXYu/zRwdUTsD8wlDSWApJeQhjV4ZR6f5hnykAMtXAC8TNLuDeV/AF6dtzMd+EJh3t7AMcAE4PPA47ne74D35DqzgeMj4gDS8A1n5vLpwJsiYj/g8BLxma3BHcpsoxIRN+fx6CcD8xpmHwz8Y653ZT4T2JY0Yubbc/nPJa3M9V8PHAAsyMMKbAHcWyKMZ4CvAKewekwoSOPd/EDSeNKAaZsW5l0VEY8Aj0hqHIFz3zxe0yuAHxeGONgs/3sN8H1JF5BGiTXrFycC2xjNJTXPHEJ6nkKPZoPERMO/RQJ+EBGnDCCGc0iJYHGh7LOkL/y35WQ1vzCv1QiczwMezGcma4iID0k6iNQktkjSSyPi/gHEbDXlpiHbGJ0FzIyIWxrKi6NJHgLcFxEPN5RPAl6Q6/8KOErSjnne9pJKjTufn+9wOvDRQvG2rL54PaU/HyjHeYekd+RYJGm//H63iLg+IqYD9wFj+rNuMycC2+hERFdEfKPJrBmkO3puJj2i8L25/DPAqyXdALyR9CQqIj1H+VTg8rzML4EX9iOU77LmWfdpwBclXcPqUTr7413AcZJuIp1p9Dxu8SvKj/ckJbWbBrBuqzGPNWRmVnM+IzAzqzknAjOzmnMiMDOrOScCM7OacyIwM6s5JwIzs5pzIjAzq7n/BzGddHAocSm4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "X_train_1,X_test_1, y_train_1, y_test_1 = train_test_split(diabetes_df, diabetes_df['Target'], test_size=0.3)\n",
    "knn_final_new = neighbors.KNeighborsClassifier(n_neighbors = 17)\n",
    "model_new = knn_final_new.fit(X_train_1, y_train_1)\n",
    "predictions_new = knn_final_new.predict(X_test_1)\n",
    "print('Accuracy on the test set is: ',metrics.accuracy_score(y_test_1, predictions_new))\n",
    "new_accuracy = metrics.accuracy_score(y_test_1, predictions_new)\n",
    "\n",
    "##VISUAL DEPICTION\n",
    "model_names = ['knn w/o imputation', 'knn w imputation']\n",
    "model_acc = [old_accuracy, new_accuracy]\n",
    "plt.bar(model_names, model_acc)\n",
    "plt.xlabel(\"Model Names\")\n",
    "plt.ylabel(\"Model Accuracies (%)\")\n",
    "plt.title(\"Comparison of Model Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Yryczkdc6nL"
   },
   "source": [
    "References\n",
    "\n",
    "Train, Test and Validation set:\n",
    "\n",
    "https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HhClSzJpyZDE"
   ],
   "name": "lab_1_v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
